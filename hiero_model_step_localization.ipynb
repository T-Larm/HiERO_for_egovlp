{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c37993c",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82b19ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone HiERO repository\n",
    "!git clone https://github.com/sapeirone/HiERO.git\n",
    "%cd HiERO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0cd02b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also clone your project repo for annotations\n",
    "!git clone https://github.com/T-Larm/aml-2025-mistake-detection-gp.git /content/aml-project\n",
    "%cd /content/aml-project\n",
    "!git submodule update --init --recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3173dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install HiERO dependencies\n",
    "%cd /content/HiERO\n",
    "!pip install -r requirements.txt -f https://data.pyg.org/whl/torch-2.4.0+cu124.html --extra-index-url https://download.pytorch.org/whl/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0072279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfe603d",
   "metadata": {},
   "source": [
    "## 2. Path Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9321dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# ================= PATH CONFIGURATION =================\n",
    "HIERO_ROOT = \"/content/HiERO\"\n",
    "PROJECT_ROOT = \"/content/aml-project\"\n",
    "DRIVE_ROOT = \"/content/drive/MyDrive/AMLproject\"\n",
    "\n",
    "# Add HiERO to Python path\n",
    "sys.path.insert(0, HIERO_ROOT)\n",
    "\n",
    "# EgoVLP features on Google Drive\n",
    "EGOVLP_FEATURES_DIR = os.path.join(DRIVE_ROOT, \"Captain_Cook_dataset/features/segments/egovlp\")\n",
    "\n",
    "# Annotations from your project\n",
    "ANNOTATIONS_PATH = os.path.join(PROJECT_ROOT, \"annotations/annotation_json/complete_step_annotations.json\")\n",
    "SPLIT_FILE = os.path.join(PROJECT_ROOT, \"er_annotations/recordings_combined_splits.json\")\n",
    "\n",
    "# HiERO pretrained model - MODIFY THIS PATH!\n",
    "# Option 1: If you have it in your HiERO_for_egovlp folder\n",
    "HIERO_CHECKPOINT = os.path.join(DRIVE_ROOT, \"HiERO_for_egovlp/hiero_egovlp/hiero_egovlp.pth\")\n",
    "# Option 2: Or download from HiERO's model zoo (see README)\n",
    "# HIERO_CHECKPOINT = \"/content/HiERO/checkpoints/hiero_egovlp.pth\"\n",
    "\n",
    "# Output directory\n",
    "OUTPUT_DIR = os.path.join(DRIVE_ROOT, \"substep1_hiero_outputs\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(\"=== Path Configuration ===\")\n",
    "print(f\"HiERO root: {HIERO_ROOT}\")\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"EgoVLP features: {EGOVLP_FEATURES_DIR}\")\n",
    "print(f\"HiERO checkpoint: {HIERO_CHECKPOINT}\")\n",
    "print(f\"Output: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f4f12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify paths\n",
    "print(\"=== Verifying Paths ===\")\n",
    "\n",
    "if os.path.exists(EGOVLP_FEATURES_DIR):\n",
    "    npz_files = [f for f in os.listdir(EGOVLP_FEATURES_DIR) if f.endswith('.npz')]\n",
    "    print(f\"‚úÖ EgoVLP features: {len(npz_files)} files\")\n",
    "else:\n",
    "    print(f\"‚ùå Features not found: {EGOVLP_FEATURES_DIR}\")\n",
    "\n",
    "if os.path.exists(ANNOTATIONS_PATH):\n",
    "    print(f\"‚úÖ Annotations found\")\n",
    "else:\n",
    "    print(f\"‚ùå Annotations not found: {ANNOTATIONS_PATH}\")\n",
    "\n",
    "if os.path.exists(HIERO_CHECKPOINT):\n",
    "    size_mb = os.path.getsize(HIERO_CHECKPOINT) / (1024*1024)\n",
    "    print(f\"‚úÖ HiERO checkpoint found ({size_mb:.1f} MB)\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è HiERO checkpoint not found: {HIERO_CHECKPOINT}\")\n",
    "    print(f\"   You may need to download it or adjust the path\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118e023e",
   "metadata": {},
   "source": [
    "## 3. Load HiERO Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5196ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from models.hiero import HiERO\n",
    "import yaml\n",
    "\n",
    "# Check GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8423c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load checkpoint to inspect structure\n",
    "checkpoint = torch.load(HIERO_CHECKPOINT, map_location='cpu')\n",
    "\n",
    "print(\"Checkpoint keys:\")\n",
    "for key in checkpoint.keys():\n",
    "    if isinstance(checkpoint[key], dict):\n",
    "        print(f\"  {key}: {len(checkpoint[key])} items\")\n",
    "    else:\n",
    "        print(f\"  {key}: {type(checkpoint[key])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e1989d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load config (if available in checkpoint, otherwise use default)\n",
    "if 'config' in checkpoint:\n",
    "    config = checkpoint['config']\n",
    "    print(\"Config loaded from checkpoint\")\n",
    "else:\n",
    "    # Load from default config file\n",
    "    config_path = os.path.join(HIERO_ROOT, 'configs/egovlp.yaml')\n",
    "    with open(config_path, 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    print(f\"Config loaded from: {config_path}\")\n",
    "\n",
    "print(\"\\nConfig structure:\")\n",
    "print(yaml.dump(config, default_flow_style=False, indent=2)[:500] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93b24c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize HiERO model\n",
    "# Extract model parameters from config\n",
    "model_config = config.get('model', {})\n",
    "\n",
    "# Build model\n",
    "model = HiERO(\n",
    "    input_size=256,  # EgoVLP feature dimension\n",
    "    **model_config\n",
    ")\n",
    "\n",
    "# Load pretrained weights\n",
    "if 'state_dict' in checkpoint:\n",
    "    state_dict = checkpoint['state_dict']\n",
    "elif 'model' in checkpoint:\n",
    "    state_dict = checkpoint['model']\n",
    "else:\n",
    "    state_dict = checkpoint\n",
    "\n",
    "# Load weights (may need to handle key mismatches)\n",
    "try:\n",
    "    model.load_state_dict(state_dict, strict=True)\n",
    "    print(\"‚úÖ Model weights loaded successfully (strict)\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Strict loading failed: {e}\")\n",
    "    print(\"Trying non-strict loading...\")\n",
    "    missing, unexpected = model.load_state_dict(state_dict, strict=False)\n",
    "    print(f\"Missing keys: {len(missing)}\")\n",
    "    print(f\"Unexpected keys: {len(unexpected)}\")\n",
    "\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(f\"\\n‚úÖ HiERO model ready on {device}\")\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()) / 1e6:.2f}M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4bbd94a",
   "metadata": {},
   "source": [
    "## 4. Load Annotations & Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0603c039",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load annotations\n",
    "with open(ANNOTATIONS_PATH, 'r') as f:\n",
    "    annotations = json.load(f)\n",
    "\n",
    "print(f\"Total annotated videos: {len(annotations)}\")\n",
    "\n",
    "# Find videos with both annotations and features\n",
    "available_features = set()\n",
    "for f in os.listdir(EGOVLP_FEATURES_DIR):\n",
    "    if f.endswith('.npz'):\n",
    "        recording_id = '_'.join(f.split('_')[:2])\n",
    "        available_features.add(recording_id)\n",
    "\n",
    "annotated_ids = set(annotations.keys())\n",
    "common_ids = annotated_ids.intersection(available_features)\n",
    "\n",
    "print(f\"Videos with both annotations and features: {len(common_ids)}\")\n",
    "print(f\"Sample IDs: {list(common_ids)[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f5aeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative: Process without annotations (if you don't have labels yet)\n",
    "USE_ANNOTATIONS = True  # Set to False to skip annotations\n",
    "\n",
    "if USE_ANNOTATIONS:\n",
    "    # Load annotations\n",
    "    with open(ANNOTATIONS_PATH, 'r') as f:\n",
    "        annotations = json.load(f)\n",
    "    print(f\"Total annotated videos: {len(annotations)}\")\n",
    "    \n",
    "    # Find videos with both annotations and features\n",
    "    available_features = set()\n",
    "    for f in os.listdir(EGOVLP_FEATURES_DIR):\n",
    "        if f.endswith('.npz'):\n",
    "            recording_id = '_'.join(f.split('_')[:2])\n",
    "            available_features.add(recording_id)\n",
    "    \n",
    "    annotated_ids = set(annotations.keys())\n",
    "    common_ids = annotated_ids.intersection(available_features)\n",
    "    print(f\"Videos with both annotations and features: {len(common_ids)}\")\n",
    "else:\n",
    "    # Process all available features without annotations\n",
    "    print(\"‚ö†Ô∏è Running without annotations - video labels will be set to -1\")\n",
    "    annotations = None\n",
    "    common_ids = set()\n",
    "    for f in os.listdir(EGOVLP_FEATURES_DIR):\n",
    "        if f.endswith('.npz'):\n",
    "            recording_id = '_'.join(f.split('_')[:2])\n",
    "            common_ids.add(recording_id)\n",
    "    print(f\"Total videos with features: {len(common_ids)}\")\n",
    "\n",
    "print(f\"Sample IDs: {list(common_ids)[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94894968",
   "metadata": {},
   "source": [
    "**Note**: Annotations are only needed for:\n",
    "1. **Video labels** (required for Substep 2 classifier training)\n",
    "2. GT boundaries (optional, for visualization/evaluation)\n",
    "\n",
    "If you only want to output step embeddings without labels, you can skip loading annotations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfdb5028",
   "metadata": {},
   "source": [
    "## 5. HiERO-based Step Localization\n",
    "\n",
    "Use HiERO's hierarchical clustering mechanism to detect step boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f652e214",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data, Batch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def process_video_with_hiero(features, model, device, n_clusters=None):\n",
    "    \"\"\"\n",
    "    Process video features through HiERO model to detect step boundaries.\n",
    "    \n",
    "    Args:\n",
    "        features: (T, 256) numpy array\n",
    "        model: HiERO model\n",
    "        device: torch device\n",
    "        n_clusters: number of steps (None for auto)\n",
    "    \n",
    "    Returns:\n",
    "        boundaries: list of (start, end) tuples\n",
    "        hierarchical_features: processed features from HiERO\n",
    "    \"\"\"\n",
    "    T, D = features.shape\n",
    "    \n",
    "    # Auto-estimate clusters\n",
    "    if n_clusters is None:\n",
    "        n_clusters = max(2, min(T // 30, 15))\n",
    "    \n",
    "    # Convert to torch tensor\n",
    "    x = torch.from_numpy(features).float().to(device)  # (T, 256)\n",
    "    \n",
    "    # Create temporal graph (connect consecutive frames)\n",
    "    edge_index = []\n",
    "    for i in range(T - 1):\n",
    "        edge_index.append([i, i + 1])\n",
    "        edge_index.append([i + 1, i])  # Bidirectional\n",
    "    \n",
    "    edge_index = torch.tensor(edge_index, dtype=torch.long).t().to(device)\n",
    "    \n",
    "    # Create PyG Data object\n",
    "    graph_data = Data(x=x, edge_index=edge_index)\n",
    "    # Add batch tensor manually (instead of using Batch.from_data_list)\n",
    "    graph_data.batch = torch.zeros(T, dtype=torch.long, device=device)\n",
    "    \n",
    "    # Forward pass through HiERO\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            # HiERO forward pass\n",
    "            output = model(graph_data)\n",
    "            \n",
    "            # Extract processed features\n",
    "            if isinstance(output, dict):\n",
    "                hierarchical_features = output.get('features', output.get('x', x))\n",
    "            else:\n",
    "                hierarchical_features = output\n",
    "            \n",
    "            hierarchical_features = hierarchical_features.cpu().numpy()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Warning: HiERO forward pass failed: {e}\")\n",
    "            print(\"Falling back to input features\")\n",
    "            hierarchical_features = features\n",
    "    \n",
    "    # Apply clustering on hierarchical features\n",
    "    from sklearn.cluster import KMeans\n",
    "    \n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "    labels = kmeans.fit_predict(hierarchical_features[:T])  # Ensure same length\n",
    "    \n",
    "    # Find boundaries where labels change\n",
    "    boundaries = []\n",
    "    current_start = 0\n",
    "    current_label = labels[0]\n",
    "    \n",
    "    for i in range(1, len(labels)):\n",
    "        if labels[i] != current_label:\n",
    "            if i - current_start >= 5:  # Min segment length\n",
    "                boundaries.append((current_start, i - 1))\n",
    "                current_start = i\n",
    "                current_label = labels[i]\n",
    "    \n",
    "    # Add last segment\n",
    "    if len(labels) - current_start >= 5:\n",
    "        boundaries.append((current_start, len(labels) - 1))\n",
    "    \n",
    "    return boundaries, hierarchical_features\n",
    "\n",
    "print(\"‚úÖ HiERO processing function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b483fbe6",
   "metadata": {},
   "source": [
    "### 5.1 Test on One Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b978f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on one video\n",
    "test_id = list(common_ids)[0]\n",
    "print(f\"Testing HiERO on: {test_id}\")\n",
    "\n",
    "# Load features\n",
    "feature_files = [f for f in os.listdir(EGOVLP_FEATURES_DIR) \n",
    "                 if f.startswith(test_id.replace('_', '_')) and f.endswith('.npz')]\n",
    "\n",
    "if feature_files:\n",
    "    feature_path = os.path.join(EGOVLP_FEATURES_DIR, feature_files[0])\n",
    "    data = np.load(feature_path)\n",
    "    features = data['arr_0']  # (T, 256)\n",
    "    \n",
    "    print(f\"Features shape: {features.shape}\")\n",
    "    print(f\"Duration: {features.shape[0]} seconds\\n\")\n",
    "    \n",
    "    # Get GT for comparison\n",
    "    gt_steps = annotations[test_id].get('steps', [])\n",
    "    print(f\"Ground truth: {len(gt_steps)} steps\")\n",
    "    \n",
    "    # Process with HiERO\n",
    "    print(\"\\nProcessing with HiERO model...\")\n",
    "    pred_boundaries, hiero_features = process_video_with_hiero(\n",
    "        features, model, device\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n‚úÖ HiERO detected: {len(pred_boundaries)} steps\")\n",
    "    for i, (start, end) in enumerate(pred_boundaries[:10]):\n",
    "        print(f\"  Step {i+1}: {start}s - {end}s ({end-start+1} frames)\")\n",
    "    \n",
    "    print(f\"\\nHierarchical features shape: {hiero_features.shape}\")\n",
    "else:\n",
    "    print(f\"No features found for {test_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983b7950",
   "metadata": {},
   "source": [
    "### 5.2 Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfff8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def compare_features_and_boundaries(original_features, hiero_features, \n",
    "                                   gt_boundaries, pred_boundaries, title=\"\"):\n",
    "    \"\"\"\n",
    "    Visualize original vs HiERO-processed features and boundaries.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(15, 9), sharex=True)\n",
    "    \n",
    "    T = original_features.shape[0]\n",
    "    time = np.arange(T)\n",
    "    \n",
    "    # Original features\n",
    "    orig_norm = np.linalg.norm(original_features, axis=1)\n",
    "    axes[0].plot(time, orig_norm, alpha=0.7, label='Original EgoVLP')\n",
    "    axes[0].set_ylabel('Feature magnitude')\n",
    "    axes[0].set_title(f'{title} - Original EgoVLP Features')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # HiERO features\n",
    "    hiero_norm = np.linalg.norm(hiero_features[:T], axis=1)\n",
    "    axes[1].plot(time, hiero_norm, alpha=0.7, color='orange', label='HiERO processed')\n",
    "    axes[1].set_ylabel('Feature magnitude')\n",
    "    axes[1].set_title('HiERO Hierarchical Features')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Boundaries comparison\n",
    "    # GT boundaries\n",
    "    for start, end in gt_boundaries:\n",
    "        axes[2].axvspan(start, end, alpha=0.15, color='green')\n",
    "        axes[2].axvline(start, color='green', linestyle='--', alpha=0.4, \n",
    "                       label='GT' if start == gt_boundaries[0][0] else '')\n",
    "    \n",
    "    # HiERO predictions\n",
    "    for start, end in pred_boundaries:\n",
    "        axes[2].axvline(start, color='red', linestyle='-', alpha=0.6, linewidth=2,\n",
    "                       label='HiERO' if start == pred_boundaries[0][0] else '')\n",
    "    \n",
    "    axes[2].plot(time, hiero_norm, alpha=0.4, color='gray')\n",
    "    axes[2].set_xlabel('Time (seconds)')\n",
    "    axes[2].set_ylabel('Feature magnitude')\n",
    "    axes[2].set_title('Boundary Comparison: GT (green) vs HiERO (red)')\n",
    "    axes[2].legend()\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize\n",
    "gt_frame_boundaries = [(int(step.get('start_time', 0)), int(step.get('end_time', 0))) \n",
    "                       for step in gt_steps]\n",
    "\n",
    "compare_features_and_boundaries(\n",
    "    features, hiero_features, \n",
    "    gt_frame_boundaries, pred_boundaries, \n",
    "    title=test_id\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471fce0f",
   "metadata": {},
   "source": [
    "## 6. Process All Videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea9669b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def process_all_videos_hiero(video_ids, model, device):\n",
    "    \"\"\"\n",
    "    Process all videos with HiERO model.\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        'boundaries': {},\n",
    "        'step_embeddings': {},\n",
    "        'video_labels': {},\n",
    "        'activity_names': {},\n",
    "    }\n",
    "    \n",
    "    for recording_id in tqdm(video_ids, desc=\"Processing with HiERO\"):\n",
    "        try:\n",
    "            # Find feature file\n",
    "            feature_files = [f for f in os.listdir(EGOVLP_FEATURES_DIR) \n",
    "                           if f.startswith(recording_id.replace('_', '_')) and f.endswith('.npz')]\n",
    "            \n",
    "            if not feature_files:\n",
    "                continue\n",
    "            \n",
    "            # Load features\n",
    "            feature_path = os.path.join(EGOVLP_FEATURES_DIR, feature_files[0])\n",
    "            data = np.load(feature_path)\n",
    "            features = data['arr_0']  # (T, 256)\n",
    "            \n",
    "            # Process with HiERO\n",
    "            boundaries, hiero_features = process_video_with_hiero(\n",
    "                features, model, device\n",
    "            )\n",
    "            \n",
    "            # Extract step embeddings using HiERO features\n",
    "            step_embeddings = []\n",
    "            for start, end in boundaries:\n",
    "                # Use HiERO-processed features for embeddings\n",
    "                step_feat = hiero_features[start:end+1].mean(axis=0)  # (256,)\n",
    "                step_embeddings.append(step_feat)\n",
    "            \n",
    "            step_embeddings = np.stack(step_embeddings, axis=0)  # (num_steps, 256)\n",
    "            \n",
    "            # Get video label\n",
    "            anno = annotations[recording_id]\n",
    "            has_errors = any(step.get('has_errors', False) for step in anno.get('steps', []))\n",
    "            video_label = 1 if has_errors else 0\n",
    "            \n",
    "            # Store results\n",
    "            results['boundaries'][recording_id] = boundaries\n",
    "            results['step_embeddings'][recording_id] = step_embeddings\n",
    "            results['video_labels'][recording_id] = video_label\n",
    "            results['activity_names'][recording_id] = anno.get('activity_name', 'unknown')\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {recording_id}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Process all videos\n",
    "print(f\"Processing {len(common_ids)} videos with HiERO model...\")\n",
    "all_results = process_all_videos_hiero(list(common_ids), model, device)\n",
    "\n",
    "print(f\"\\n‚úÖ Successfully processed {len(all_results['boundaries'])} videos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7da0f3",
   "metadata": {},
   "source": [
    "## 7. Statistics & Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6896d23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistics\n",
    "num_steps_list = [len(bounds) for bounds in all_results['boundaries'].values()]\n",
    "labels = list(all_results['video_labels'].values())\n",
    "\n",
    "print(\"=== HiERO-based Substep 1 Results ===\")\n",
    "print(f\"Total videos processed: {len(all_results['boundaries'])}\")\n",
    "print(f\"\\nVideo labels distribution:\")\n",
    "print(f\"  - Correct executions (label=0): {labels.count(0)} ({labels.count(0)/len(labels)*100:.1f}%)\")\n",
    "print(f\"  - Has errors (label=1): {labels.count(1)} ({labels.count(1)/len(labels)*100:.1f}%)\")\n",
    "print(f\"\\nStep detection statistics:\")\n",
    "print(f\"  - Avg steps per video: {np.mean(num_steps_list):.1f}\")\n",
    "print(f\"  - Min/Max steps: {min(num_steps_list)} / {max(num_steps_list)}\")\n",
    "print(f\"  - Median steps: {np.median(num_steps_list):.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b371dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].hist(num_steps_list, bins=20, edgecolor='black', alpha=0.7, color='orange')\n",
    "axes[0].set_xlabel('Number of steps')\n",
    "axes[0].set_ylabel('Number of videos')\n",
    "axes[0].set_title('HiERO: Distribution of Steps per Video')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "label_counts = [labels.count(0), labels.count(1)]\n",
    "axes[1].bar(['Correct (0)', 'Has Errors (1)'], label_counts, color=['green', 'red'], alpha=0.7)\n",
    "axes[1].set_ylabel('Number of videos')\n",
    "axes[1].set_title('Video Label Distribution')\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0320557",
   "metadata": {},
   "source": [
    "## 8. Prepare Data for Teammates (Substeps 2, 3, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617d3552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare padded embeddings for Substep 2\n",
    "max_steps = max(len(bounds) for bounds in all_results['boundaries'].values())\n",
    "print(f\"Max steps in dataset: {max_steps}\")\n",
    "\n",
    "# Create padded arrays\n",
    "recording_ids = list(all_results['step_embeddings'].keys())\n",
    "n_videos = len(recording_ids)\n",
    "\n",
    "padded_embeddings = np.zeros((n_videos, max_steps, 256), dtype=np.float32)\n",
    "masks = np.zeros((n_videos, max_steps), dtype=bool)\n",
    "video_labels = np.zeros(n_videos, dtype=np.int64)\n",
    "\n",
    "for i, rec_id in enumerate(recording_ids):\n",
    "    embeddings = all_results['step_embeddings'][rec_id]\n",
    "    num_steps = len(embeddings)\n",
    "    \n",
    "    padded_embeddings[i, :num_steps] = embeddings\n",
    "    masks[i, :num_steps] = True\n",
    "    video_labels[i] = all_results['video_labels'][rec_id]\n",
    "\n",
    "print(f\"\\n=== Data for Substep 2 (HiERO-based) ===\")\n",
    "print(f\"Embeddings shape: {padded_embeddings.shape}\")\n",
    "print(f\"Masks shape: {masks.shape}\")\n",
    "print(f\"Labels shape: {video_labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd88595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save for Substep 2\n",
    "substep2_output = os.path.join(OUTPUT_DIR, \"hiero_substep2_data.npz\")\n",
    "np.savez(\n",
    "    substep2_output,\n",
    "    embeddings=padded_embeddings,\n",
    "    masks=masks,\n",
    "    labels=video_labels,\n",
    "    recording_ids=np.array(recording_ids, dtype=object),\n",
    "    max_steps=max_steps\n",
    ")\n",
    "print(f\"‚úÖ Saved: {substep2_output}\")\n",
    "print(f\"   Size: {os.path.getsize(substep2_output) / (1024*1024):.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b50758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save boundaries for Substeps 3 & 4\n",
    "substep3_output = os.path.join(OUTPUT_DIR, \"hiero_step_boundaries.json\")\n",
    "\n",
    "boundaries_json = {\n",
    "    rec_id: {\n",
    "        'boundaries': [(int(s), int(e)) for s, e in bounds],\n",
    "        'num_steps': len(bounds),\n",
    "        'video_label': int(all_results['video_labels'][rec_id]),\n",
    "        'activity': all_results['activity_names'][rec_id]\n",
    "    }\n",
    "    for rec_id, bounds in all_results['boundaries'].items()\n",
    "}\n",
    "\n",
    "with open(substep3_output, 'w') as f:\n",
    "    json.dump(boundaries_json, f, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Saved: {substep3_output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab3661e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save step embeddings\n",
    "substep3_embeddings = os.path.join(OUTPUT_DIR, \"hiero_step_embeddings.npz\")\n",
    "\n",
    "np.savez(\n",
    "    substep3_embeddings,\n",
    "    **{rec_id: emb for rec_id, emb in all_results['step_embeddings'].items()}\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Saved: {substep3_embeddings}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f5f177",
   "metadata": {},
   "source": [
    "## 9. Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5f2d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"EXTENSION SUBSTEP 1 COMPLETE (HiERO-based)!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nüìä Processing Summary:\")\n",
    "print(f\"   - Method: HiERO hierarchical model\")\n",
    "print(f\"   - Videos processed: {len(all_results['boundaries'])}\")\n",
    "print(f\"   - Total steps detected: {sum(num_steps_list)}\")\n",
    "print(f\"   - Avg steps per video: {np.mean(num_steps_list):.1f}\")\n",
    "\n",
    "print(f\"\\nüìÅ Output files in: {OUTPUT_DIR}\")\n",
    "for fname in ['hiero_substep2_data.npz', 'hiero_step_boundaries.json', 'hiero_step_embeddings.npz']:\n",
    "    fpath = os.path.join(OUTPUT_DIR, fname)\n",
    "    if os.path.exists(fpath):\n",
    "        size_mb = os.path.getsize(fpath) / (1024*1024)\n",
    "        print(f\"   ‚úÖ {fname} ({size_mb:.2f} MB)\")\n",
    "\n",
    "print(f\"\\nüéØ Deliverables ready for Substeps 2, 3, 4!\")\n",
    "print(f\"\\nüí° Advantages of HiERO-based approach:\")\n",
    "print(f\"   - Hierarchical understanding of recipe steps\")\n",
    "print(f\"   - Temporal context modeling via Graph U-Net\")\n",
    "print(f\"   - Features trained on video-text alignment\")\n",
    "print(f\"   - Better semantic grouping of actions\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
