{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da82ae8e",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8f6462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repository\n",
    "!git clone https://github.com/YOUR_REPO/HiERO_for_egovlp.git\n",
    "%cd HiERO_for_egovlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0ca9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -r requirements.txt -f https://data.pyg.org/whl/torch-2.4.0+cu124.html --extra-index-url https://download.pytorch.org/whl/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39454f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42092a27",
   "metadata": {},
   "source": [
    "## 2. Path Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4699460b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# ================= MODIFY THESE PATHS =================\n",
    "PROJECT_ROOT = \"/content/HiERO_for_egovlp\"\n",
    "DRIVE_ROOT = \"/content/drive/MyDrive/AMLproject\"\n",
    "\n",
    "# EgoVLP features (from your screenshot)\n",
    "EGOVLP_FEATURES_DIR = os.path.join(DRIVE_ROOT, \"Captain_Cook_dataset/features/segments/egovlp\")\n",
    "\n",
    "# Annotations\n",
    "ANNOTATIONS_PATH = os.path.join(PROJECT_ROOT, \"annotations/annotation_json/complete_step_annotations.json\")\n",
    "SPLIT_FILE = os.path.join(PROJECT_ROOT, \"er_annotations/recordings_combined_splits.json\")\n",
    "\n",
    "# Output directory (save to Drive for persistence)\n",
    "OUTPUT_DIR = os.path.join(DRIVE_ROOT, \"substep1_outputs\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(\"=== Paths ===\")\n",
    "print(f\"EgoVLP features: {EGOVLP_FEATURES_DIR}\")\n",
    "print(f\"Annotations: {ANNOTATIONS_PATH}\")\n",
    "print(f\"Output: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a56a046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify paths\n",
    "import os\n",
    "\n",
    "print(\"=== Verifying Paths ===\")\n",
    "if os.path.exists(EGOVLP_FEATURES_DIR):\n",
    "    npz_files = [f for f in os.listdir(EGOVLP_FEATURES_DIR) if f.endswith('.npz')]\n",
    "    print(f\"âœ… EgoVLP features: {len(npz_files)} files\")\n",
    "else:\n",
    "    print(f\"âŒ Features not found: {EGOVLP_FEATURES_DIR}\")\n",
    "\n",
    "if os.path.exists(ANNOTATIONS_PATH):\n",
    "    print(f\"âœ… Annotations found\")\n",
    "else:\n",
    "    print(f\"âŒ Annotations not found: {ANNOTATIONS_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7f0cab",
   "metadata": {},
   "source": [
    "## 3. Load Annotations & Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420d6119",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# Load annotations\n",
    "with open(ANNOTATIONS_PATH, 'r') as f:\n",
    "    annotations = json.load(f)\n",
    "\n",
    "print(f\"Total annotated videos: {len(annotations)}\")\n",
    "\n",
    "# Sample one annotation\n",
    "sample_id = list(annotations.keys())[0]\n",
    "sample = annotations[sample_id]\n",
    "print(f\"\\nSample video: {sample_id}\")\n",
    "print(f\"Keys: {sample.keys()}\")\n",
    "if 'steps' in sample:\n",
    "    print(f\"Number of steps: {len(sample['steps'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3baa92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load split information (train/val/test)\n",
    "with open(SPLIT_FILE, 'r') as f:\n",
    "    splits = json.load(f)\n",
    "\n",
    "print(f\"Train: {len(splits.get('train', []))} videos\")\n",
    "print(f\"Val: {len(splits.get('val', []))} videos\")\n",
    "print(f\"Test: {len(splits.get('test', []))} videos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e290be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find videos with both annotations and features\n",
    "available_features = set()\n",
    "for f in os.listdir(EGOVLP_FEATURES_DIR):\n",
    "    if f.endswith('.npz'):\n",
    "        # Extract recording_id: \"9_8_360p_224.mp4_1s_1s.npz\" -> \"9_8\"\n",
    "        recording_id = '_'.join(f.split('_')[:2])\n",
    "        available_features.add(recording_id)\n",
    "\n",
    "annotated_ids = set(annotations.keys())\n",
    "common_ids = annotated_ids.intersection(available_features)\n",
    "\n",
    "print(f\"\\nVideos with both annotations and features: {len(common_ids)}\")\n",
    "print(f\"Sample IDs: {list(common_ids)[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bf6e2e",
   "metadata": {},
   "source": [
    "## 4. Method 1: Zero-shot Clustering-based Step Localization\n",
    "\n",
    "Use temporal clustering on EgoVLP features to detect step boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42af9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.signal import find_peaks\n",
    "import numpy as np\n",
    "\n",
    "def detect_boundaries_clustering(features, n_clusters='auto', min_cluster_size=5):\n",
    "    \"\"\"\n",
    "    Detect step boundaries using temporal clustering.\n",
    "    \n",
    "    Args:\n",
    "        features: (T, D) array of frame-level features\n",
    "        n_clusters: number of clusters (steps), 'auto' to estimate\n",
    "        min_cluster_size: minimum frames per cluster\n",
    "    \n",
    "    Returns:\n",
    "        boundaries: list of (start, end) frame indices\n",
    "    \"\"\"\n",
    "    T, D = features.shape\n",
    "    \n",
    "    # Auto-estimate number of clusters if needed\n",
    "    if n_clusters == 'auto':\n",
    "        # Heuristic: ~30 seconds per step at 1 FPS\n",
    "        n_clusters = max(2, min(T // 30, 15))  # 2-15 steps\n",
    "    \n",
    "    # K-means clustering\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "    labels = kmeans.fit_predict(features)\n",
    "    \n",
    "    # Find boundary points (where label changes)\n",
    "    boundaries = []\n",
    "    current_start = 0\n",
    "    current_label = labels[0]\n",
    "    \n",
    "    for i in range(1, len(labels)):\n",
    "        if labels[i] != current_label:\n",
    "            # Check minimum size\n",
    "            if i - current_start >= min_cluster_size:\n",
    "                boundaries.append((current_start, i - 1))\n",
    "                current_start = i\n",
    "                current_label = labels[i]\n",
    "    \n",
    "    # Add last segment\n",
    "    if T - current_start >= min_cluster_size:\n",
    "        boundaries.append((current_start, T - 1))\n",
    "    \n",
    "    return boundaries\n",
    "\n",
    "def detect_boundaries_similarity(features, threshold=0.5, min_segment_length=5):\n",
    "    \"\"\"\n",
    "    Alternative: Detect boundaries using cosine similarity changes.\n",
    "    \"\"\"\n",
    "    T, D = features.shape\n",
    "    \n",
    "    # Compute cosine similarity between consecutive frames\n",
    "    features_norm = features / (np.linalg.norm(features, axis=1, keepdims=True) + 1e-8)\n",
    "    similarities = np.sum(features_norm[:-1] * features_norm[1:], axis=1)\n",
    "    \n",
    "    # Find dips in similarity (potential boundaries)\n",
    "    dissimilarity = 1 - similarities\n",
    "    peaks, _ = find_peaks(dissimilarity, height=threshold, distance=min_segment_length)\n",
    "    \n",
    "    # Convert peaks to boundaries\n",
    "    boundaries = []\n",
    "    start = 0\n",
    "    for peak in peaks:\n",
    "        boundaries.append((start, peak))\n",
    "        start = peak + 1\n",
    "    boundaries.append((start, T - 1))\n",
    "    \n",
    "    return boundaries\n",
    "\n",
    "print(\"âœ… Boundary detection functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b343e5cb",
   "metadata": {},
   "source": [
    "### 4.1 Test on One Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35cfca8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on one video\n",
    "test_id = list(common_ids)[0]\n",
    "print(f\"Testing on: {test_id}\")\n",
    "\n",
    "# Load features\n",
    "feature_files = [f for f in os.listdir(EGOVLP_FEATURES_DIR) \n",
    "                 if f.startswith(test_id.replace('_', '_')) and f.endswith('.npz')]\n",
    "if feature_files:\n",
    "    feature_path = os.path.join(EGOVLP_FEATURES_DIR, feature_files[0])\n",
    "    data = np.load(feature_path)\n",
    "    features = data['arr_0']  # Shape: (T, 256)\n",
    "    \n",
    "    print(f\"Features shape: {features.shape}\")\n",
    "    print(f\"Duration: {features.shape[0]} seconds (at 1 FPS)\")\n",
    "    \n",
    "    # Get ground truth\n",
    "    gt_steps = annotations[test_id].get('steps', [])\n",
    "    print(f\"\\nGround truth: {len(gt_steps)} steps\")\n",
    "    for i, step in enumerate(gt_steps[:5]):\n",
    "        print(f\"  [{i+1}] {step.get('start_time', 0):.1f}s - {step.get('end_time', 0):.1f}s: {step.get('description', '')[:50]}\")\n",
    "    \n",
    "    # Predict boundaries\n",
    "    pred_boundaries = detect_boundaries_clustering(features, n_clusters='auto')\n",
    "    print(f\"\\nPredicted: {len(pred_boundaries)} steps\")\n",
    "    for i, (start, end) in enumerate(pred_boundaries[:5]):\n",
    "        print(f\"  [{i+1}] {start}s - {end}s ({end-start+1} frames)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc7da35",
   "metadata": {},
   "source": [
    "### 4.2 Visualize Predictions vs Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3dc39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_boundaries(features, gt_boundaries, pred_boundaries, title=\"\"):\n",
    "    \"\"\"\n",
    "    Visualize feature timeline with GT and predicted boundaries.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(15, 6), sharex=True)\n",
    "    \n",
    "    T = features.shape[0]\n",
    "    time = np.arange(T)\n",
    "    \n",
    "    # Plot feature magnitude\n",
    "    feature_norm = np.linalg.norm(features, axis=1)\n",
    "    axes[0].plot(time, feature_norm, alpha=0.6, label='Feature magnitude')\n",
    "    axes[0].set_ylabel('Feature norm')\n",
    "    axes[0].set_title(f'{title} - Feature timeline')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Mark GT boundaries\n",
    "    for start, end in gt_boundaries:\n",
    "        axes[0].axvspan(start, end, alpha=0.2, color='green', label='GT step' if start == gt_boundaries[0][0] else '')\n",
    "        axes[0].axvline(start, color='green', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    # Mark predicted boundaries\n",
    "    for start, end in pred_boundaries:\n",
    "        axes[1].axvspan(start, end, alpha=0.2, color='blue', label='Predicted step' if start == pred_boundaries[0][0] else '')\n",
    "        axes[1].axvline(start, color='blue', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    axes[1].plot(time, feature_norm, alpha=0.6)\n",
    "    axes[1].set_xlabel('Time (seconds)')\n",
    "    axes[1].set_ylabel('Feature norm')\n",
    "    axes[1].set_title('Predicted boundaries')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[0].legend()\n",
    "    axes[1].legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Convert GT to frame indices (assuming 1 FPS)\n",
    "gt_frame_boundaries = [(int(step.get('start_time', 0)), int(step.get('end_time', 0))) \n",
    "                       for step in gt_steps]\n",
    "\n",
    "visualize_boundaries(features, gt_frame_boundaries, pred_boundaries, title=test_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4859db79",
   "metadata": {},
   "source": [
    "## 5. Process All Videos & Extract Step Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7b9a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def process_all_videos(video_ids, method='clustering'):\n",
    "    \"\"\"\n",
    "    Process all videos to get step boundaries and embeddings.\n",
    "    \n",
    "    Returns:\n",
    "        results: dict with keys:\n",
    "            - 'boundaries': {recording_id: [(start, end), ...]}\n",
    "            - 'step_embeddings': {recording_id: (num_steps, 256)}\n",
    "            - 'video_labels': {recording_id: 0/1}\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        'boundaries': {},\n",
    "        'step_embeddings': {},\n",
    "        'video_labels': {},\n",
    "        'activity_names': {},\n",
    "    }\n",
    "    \n",
    "    for recording_id in tqdm(video_ids, desc=\"Processing videos\"):\n",
    "        try:\n",
    "            # Find feature file\n",
    "            feature_files = [f for f in os.listdir(EGOVLP_FEATURES_DIR) \n",
    "                           if f.startswith(recording_id.replace('_', '_')) and f.endswith('.npz')]\n",
    "            \n",
    "            if not feature_files:\n",
    "                continue\n",
    "            \n",
    "            # Load features\n",
    "            feature_path = os.path.join(EGOVLP_FEATURES_DIR, feature_files[0])\n",
    "            data = np.load(feature_path)\n",
    "            features = data['arr_0']  # (T, 256)\n",
    "            \n",
    "            # Detect boundaries\n",
    "            if method == 'clustering':\n",
    "                boundaries = detect_boundaries_clustering(features)\n",
    "            else:\n",
    "                boundaries = detect_boundaries_similarity(features)\n",
    "            \n",
    "            # Extract step embeddings (average features in each segment)\n",
    "            step_embeddings = []\n",
    "            for start, end in boundaries:\n",
    "                step_feat = features[start:end+1].mean(axis=0)  # (256,)\n",
    "                step_embeddings.append(step_feat)\n",
    "            \n",
    "            step_embeddings = np.stack(step_embeddings, axis=0)  # (num_steps, 256)\n",
    "            \n",
    "            # Get video label (has errors or not)\n",
    "            anno = annotations[recording_id]\n",
    "            has_errors = any(step.get('has_errors', False) for step in anno.get('steps', []))\n",
    "            video_label = 1 if has_errors else 0\n",
    "            \n",
    "            # Store results\n",
    "            results['boundaries'][recording_id] = boundaries\n",
    "            results['step_embeddings'][recording_id] = step_embeddings\n",
    "            results['video_labels'][recording_id] = video_label\n",
    "            results['activity_names'][recording_id] = anno.get('activity_name', 'unknown')\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {recording_id}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Process all available videos\n",
    "print(f\"Processing {len(common_ids)} videos...\")\n",
    "all_results = process_all_videos(list(common_ids), method='clustering')\n",
    "\n",
    "print(f\"\\nâœ… Processed {len(all_results['boundaries'])} videos successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867c069b",
   "metadata": {},
   "source": [
    "## 6. Statistics & Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93259b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistics\n",
    "num_steps_list = [len(bounds) for bounds in all_results['boundaries'].values()]\n",
    "labels = list(all_results['video_labels'].values())\n",
    "\n",
    "print(\"=== Substep 1 Results ===\")\n",
    "print(f\"Total videos processed: {len(all_results['boundaries'])}\")\n",
    "print(f\"\\nVideo labels distribution:\")\n",
    "print(f\"  - Correct executions (label=0): {labels.count(0)} ({labels.count(0)/len(labels)*100:.1f}%)\")\n",
    "print(f\"  - Has errors (label=1): {labels.count(1)} ({labels.count(1)/len(labels)*100:.1f}%)\")\n",
    "print(f\"\\nStep detection statistics:\")\n",
    "print(f\"  - Avg steps per video: {np.mean(num_steps_list):.1f}\")\n",
    "print(f\"  - Min/Max steps: {min(num_steps_list)} / {max(num_steps_list)}\")\n",
    "print(f\"  - Median steps: {np.median(num_steps_list):.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e531f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize distribution\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Steps distribution\n",
    "axes[0].hist(num_steps_list, bins=20, edgecolor='black', alpha=0.7)\n",
    "axes[0].set_xlabel('Number of steps')\n",
    "axes[0].set_ylabel('Number of videos')\n",
    "axes[0].set_title('Distribution of Steps per Video')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Label distribution\n",
    "label_counts = [labels.count(0), labels.count(1)]\n",
    "axes[1].bar(['Correct (0)', 'Has Errors (1)'], label_counts, color=['green', 'red'], alpha=0.7)\n",
    "axes[1].set_ylabel('Number of videos')\n",
    "axes[1].set_title('Video Label Distribution')\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d019c0f9",
   "metadata": {},
   "source": [
    "## 7. Prepare Data for Teammates (Substeps 2, 3, 4)\n",
    "\n",
    "Format the output data for downstream tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d22c8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare padded embeddings for Substep 2 (Transformer classifier)\n",
    "max_steps = max(len(bounds) for bounds in all_results['boundaries'].values())\n",
    "print(f\"Max steps in dataset: {max_steps}\")\n",
    "\n",
    "# Create padded arrays\n",
    "recording_ids = list(all_results['step_embeddings'].keys())\n",
    "n_videos = len(recording_ids)\n",
    "\n",
    "padded_embeddings = np.zeros((n_videos, max_steps, 256), dtype=np.float32)\n",
    "masks = np.zeros((n_videos, max_steps), dtype=bool)\n",
    "video_labels = np.zeros(n_videos, dtype=np.int64)\n",
    "\n",
    "for i, rec_id in enumerate(recording_ids):\n",
    "    embeddings = all_results['step_embeddings'][rec_id]  # (num_steps, 256)\n",
    "    num_steps = len(embeddings)\n",
    "    \n",
    "    # Fill in embeddings and mask\n",
    "    padded_embeddings[i, :num_steps] = embeddings\n",
    "    masks[i, :num_steps] = True\n",
    "    video_labels[i] = all_results['video_labels'][rec_id]\n",
    "\n",
    "print(f\"\\n=== Data for Substep 2 (Task Verification) ===\")\n",
    "print(f\"Embeddings shape: {padded_embeddings.shape}  # (N, max_steps, 256)\")\n",
    "print(f\"Masks shape: {masks.shape}  # (N, max_steps)\")\n",
    "print(f\"Labels shape: {video_labels.shape}  # (N,)\")\n",
    "print(f\"\\nThis can be fed to a Transformer classifier for video-level prediction.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c461cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save for Substep 2\n",
    "substep2_output = os.path.join(OUTPUT_DIR, \"substep2_data.npz\")\n",
    "np.savez(\n",
    "    substep2_output,\n",
    "    embeddings=padded_embeddings,\n",
    "    masks=masks,\n",
    "    labels=video_labels,\n",
    "    recording_ids=np.array(recording_ids, dtype=object),\n",
    "    max_steps=max_steps\n",
    ")\n",
    "print(f\"âœ… Saved Substep 2 data: {substep2_output}\")\n",
    "print(f\"   Size: {os.path.getsize(substep2_output) / (1024*1024):.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3a5338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save boundaries for Substeps 3 & 4 (Task Graph matching)\n",
    "substep3_output = os.path.join(OUTPUT_DIR, \"step_boundaries.json\")\n",
    "\n",
    "# Convert numpy arrays to lists for JSON serialization\n",
    "boundaries_json = {\n",
    "    rec_id: {\n",
    "        'boundaries': [(int(s), int(e)) for s, e in bounds],\n",
    "        'num_steps': len(bounds),\n",
    "        'video_label': int(all_results['video_labels'][rec_id]),\n",
    "        'activity': all_results['activity_names'][rec_id]\n",
    "    }\n",
    "    for rec_id, bounds in all_results['boundaries'].items()\n",
    "}\n",
    "\n",
    "with open(substep3_output, 'w') as f:\n",
    "    json.dump(boundaries_json, f, indent=2)\n",
    "\n",
    "print(f\"âœ… Saved step boundaries: {substep3_output}\")\n",
    "print(f\"   This can be used for task graph matching in Substeps 3 & 4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81c9a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save raw step embeddings (for Substep 3)\n",
    "substep3_embeddings = os.path.join(OUTPUT_DIR, \"step_embeddings.npz\")\n",
    "\n",
    "# Save as dict of arrays\n",
    "np.savez(\n",
    "    substep3_embeddings,\n",
    "    **{rec_id: emb for rec_id, emb in all_results['step_embeddings'].items()}\n",
    ")\n",
    "\n",
    "print(f\"âœ… Saved step embeddings: {substep3_embeddings}\")\n",
    "print(f\"   Shape per video: (num_steps, 256)\")\n",
    "print(f\"   Use for matching with task graph node embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4380361",
   "metadata": {},
   "source": [
    "## 8. Summary & Deliverables\n",
    "\n",
    "### âœ… What you've completed:\n",
    "\n",
    "1. **Step Localization**: Detected step boundaries using zero-shot clustering on EgoVLP features\n",
    "2. **Step Embeddings**: Extracted step-level embeddings by averaging features in each segment\n",
    "3. **Data Preparation**: Prepared outputs in the format needed by teammates\n",
    "\n",
    "### ðŸ“¦ Deliverables for teammates:\n",
    "\n",
    "**For Substep 2 (Task Verification baseline)**:\n",
    "- `substep2_data.npz`:\n",
    "  - `embeddings`: (N, max_steps, 256) - padded step embeddings\n",
    "  - `masks`: (N, max_steps) - valid step positions\n",
    "  - `labels`: (N,) - video-level correctness labels\n",
    "  - Use this to train a Transformer classifier\n",
    "\n",
    "**For Substep 3 (Task Graph encoding)**:\n",
    "- `step_boundaries.json`: Step boundaries for each video\n",
    "- `step_embeddings.npz`: Variable-length step embeddings per video\n",
    "- Use these to match detected steps with task graph nodes\n",
    "\n",
    "**For Substep 4 (GNN classifier)**:\n",
    "- Same files as Substep 3\n",
    "- After matching, construct task graph with updated node features\n",
    "- Feed to GNN for classification\n",
    "\n",
    "### ðŸ”„ Next steps:\n",
    "\n",
    "1. Share the output files with teammates\n",
    "2. Optionally: Compare with ActionFormer predictions (if available)\n",
    "3. Optionally: Tune clustering parameters for better boundary detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dda8d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"=\"*60)\n",
    "print(\"EXTENSION SUBSTEP 1 COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nðŸ“Š Processing Summary:\")\n",
    "print(f\"   - Videos processed: {len(all_results['boundaries'])}\")\n",
    "print(f\"   - Total steps detected: {sum(num_steps_list)}\")\n",
    "print(f\"   - Avg steps per video: {np.mean(num_steps_list):.1f}\")\n",
    "print(f\"\\nðŸ“ Output files in: {OUTPUT_DIR}\")\n",
    "for fname in ['substep2_data.npz', 'step_boundaries.json', 'step_embeddings.npz']:\n",
    "    fpath = os.path.join(OUTPUT_DIR, fname)\n",
    "    if os.path.exists(fpath):\n",
    "        size_mb = os.path.getsize(fpath) / (1024*1024)\n",
    "        print(f\"   âœ… {fname} ({size_mb:.2f} MB)\")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Ready for teammates to implement Substeps 2, 3, 4!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c72fa9b",
   "metadata": {},
   "source": [
    "## Optional: Evaluate Against Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b567e856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Compute IoU/F1 metrics against ground truth boundaries\n",
    "def compute_iou(pred_bounds, gt_bounds, video_length):\n",
    "    \"\"\"\n",
    "    Compute IoU between predicted and ground truth boundaries.\n",
    "    \"\"\"\n",
    "    pred_mask = np.zeros(video_length, dtype=bool)\n",
    "    gt_mask = np.zeros(video_length, dtype=bool)\n",
    "    \n",
    "    for start, end in pred_bounds:\n",
    "        pred_mask[start:end+1] = True\n",
    "    \n",
    "    for start, end in gt_bounds:\n",
    "        gt_mask[start:end+1] = True\n",
    "    \n",
    "    intersection = np.logical_and(pred_mask, gt_mask).sum()\n",
    "    union = np.logical_or(pred_mask, gt_mask).sum()\n",
    "    \n",
    "    return intersection / union if union > 0 else 0.0\n",
    "\n",
    "# Compute average IoU\n",
    "ious = []\n",
    "for rec_id in list(all_results['boundaries'].keys())[:10]:  # Sample\n",
    "    pred_bounds = all_results['boundaries'][rec_id]\n",
    "    \n",
    "    gt_steps = annotations[rec_id].get('steps', [])\n",
    "    gt_bounds = [(int(s.get('start_time', 0)), int(s.get('end_time', 0))) for s in gt_steps]\n",
    "    \n",
    "    # Get video length from features\n",
    "    feature_files = [f for f in os.listdir(EGOVLP_FEATURES_DIR) \n",
    "                    if f.startswith(rec_id.replace('_', '_')) and f.endswith('.npz')]\n",
    "    if feature_files:\n",
    "        data = np.load(os.path.join(EGOVLP_FEATURES_DIR, feature_files[0]))\n",
    "        video_length = data['arr_0'].shape[0]\n",
    "        \n",
    "        iou = compute_iou(pred_bounds, gt_bounds, video_length)\n",
    "        ious.append(iou)\n",
    "\n",
    "if ious:\n",
    "    print(f\"\\nAverage IoU (sample): {np.mean(ious):.3f}\")\n",
    "    print(\"Note: This is just for reference. The important output is the step embeddings.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
