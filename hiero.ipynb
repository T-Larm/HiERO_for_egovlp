{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f749f8ce",
   "metadata": {},
   "source": [
    "# HiERO Step Localization (Clean Version)\n",
    "\n",
    "**Function**: Use the HiERO model to perform step segmentation on EgoVLP features.\n",
    "**Features**:\n",
    "1. Fixed environment dependency conflicts (Hydra/NetworkX).\n",
    "2. Used advanced Spectral Clustering algorithm.\n",
    "3. Automatically handled PyTorch `weights_only` security error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f818ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Environment Setup\n",
    "import os\n",
    "\n",
    "# Clone HiERO repository\n",
    "if not os.path.exists('/content/HiERO'):\n",
    "    !git clone https://github.com/T-Larm/HiERO_for_egovlp.git /content/HiERO\n",
    "\n",
    "# Install core dependencies (using verified stable versions)\n",
    "print(\"üì¶ Installing dependencies...\")\n",
    "!pip install -q hydra-core omegaconf einops torch_kmeans networkx\n",
    "\n",
    "# Install PyTorch Geometric (auto-match current Colab Torch version)\n",
    "!pip install -q torch-geometric\n",
    "!pip install -q torch-scatter torch-sparse -f https://data.pyg.org/whl/torch-2.5.1+cu124.html\n",
    "\n",
    "print(\"‚úÖ Environment installation complete! Please proceed to the next step.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50de4db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Mount Drive and Path Configuration\n",
    "import sys\n",
    "from google.colab import drive\n",
    "\n",
    "# Mount Google Drive\n",
    "if not os.path.exists('/content/drive'):\n",
    "    drive.mount('/content/drive', force_remount=True)\n",
    "\n",
    "# Add HiERO to system path\n",
    "if '/content/HiERO' not in sys.path:\n",
    "    sys.path.insert(0, '/content/HiERO')\n",
    "\n",
    "# ================= Path Configuration =================\n",
    "# 1. Annotation file path (If in Drive, modify here; otherwise, auto-clone project)\n",
    "PROJECT_ROOT = \"/content/aml-2025-mistake-detection-gp\"\n",
    "if not os.path.exists(PROJECT_ROOT):\n",
    "    !git clone https://github.com/T-Larm/aml-2025-mistake-detection-gp.git $PROJECT_ROOT\n",
    "\n",
    "ANNOTATIONS_PATH = os.path.join(PROJECT_ROOT, \"annotations/annotation_json/complete_step_annotations.json\")\n",
    "\n",
    "# 2. Feature and model paths (Based on your description)\n",
    "EGOVLP_FEATURES_DIR = \"/content/drive/MyDrive/AMLproject/our_features/gopro/segments/egovlp\"\n",
    "HIERO_CHECKPOINT = \"/content/drive/MyDrive/AMLproject/Captain_Cook_dataset/hiero_egovlp/hiero_egovlp.pth\"\n",
    "OUTPUT_DIR = \"/content/drive/MyDrive/AMLproject/extension1_outputs\"\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "print(f\"üìÇ Reading features: {EGOVLP_FEATURES_DIR}\")\n",
    "print(f\"üìÇ Output results: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebab921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Load HiERO Model (Fixed Version)\n",
    "import torch\n",
    "import yaml\n",
    "import hydra\n",
    "from models.hiero import HiERO\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if os.path.exists(HIERO_CHECKPOINT):\n",
    "    print(\"üöÄ Loading model...\")\n",
    "    \n",
    "    # Read Checkpoint (Disable weights_only security check to prevent errors)\n",
    "    try:\n",
    "        checkpoint = torch.load(HIERO_CHECKPOINT, map_location='cpu', weights_only=False)\n",
    "    except TypeError:\n",
    "        checkpoint = torch.load(HIERO_CHECKPOINT, map_location='cpu')\n",
    "    \n",
    "    # Process configuration\n",
    "    if 'config' in checkpoint:\n",
    "        config = checkpoint['config']\n",
    "    else:\n",
    "        config = {'model': {'conv': 'TDGC', 'hidden_size': 256, 'k': 2.0, 'n_layers': 2}}\n",
    "\n",
    "    model_config = config.get('model', {}).copy()\n",
    "    \n",
    "    # Clean up Hydra residual parameters\n",
    "    for key in ['_target_', '_recursive_', '_convert_']:\n",
    "        model_config.pop(key, None)\n",
    "    \n",
    "    if isinstance(model_config.get('conv'), str):\n",
    "        model_config['conv'] = {'name': model_config['conv']}\n",
    "\n",
    "    # Initialize\n",
    "    hiero_model = HiERO(input_size=256, **model_config)\n",
    "    \n",
    "    # Load weights\n",
    "    state_dict = checkpoint.get('state_dict', checkpoint.get('model', checkpoint))\n",
    "    new_state_dict = {k.replace('module.', ''): v for k, v in state_dict.items()}\n",
    "    hiero_model.load_state_dict(new_state_dict, strict=False)\n",
    "    \n",
    "    hiero_model = hiero_model.to(device).eval()\n",
    "    print(\"‚úÖ HiERO model loaded successfully!\")\n",
    "else:\n",
    "    print(f\"‚ùå Model file not found: {HIERO_CHECKPOINT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab34cd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Define core segmentation function (includes Spectral Clustering and post-processing)\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from torch_geometric.data import Data\n",
    "import numpy as np\n",
    "\n",
    "def detect_steps_with_hiero(features, model, device, n_clusters='auto', use_spectral=True, stride_seconds=None, fps=30):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        stride_seconds: Feature extraction stride in seconds (e.g., 1.0 for 1s). If None, uses stride=16 frames.\n",
    "        fps: Video frame rate (default: 30)\n",
    "    \"\"\"\n",
    "    T, D = features.shape\n",
    "\n",
    "    # 1. Auto-estimate number of clusters\n",
    "    if n_clusters == 'auto':\n",
    "        n_clusters = max(3, min(T // 30, 15))\n",
    "        n_clusters = int(n_clusters)\n",
    "\n",
    "    # 2. Construct graph data (with all required attributes for HiERO)\n",
    "    x = torch.from_numpy(features).float().to(device)\n",
    "    edge_index = []\n",
    "    for i in range(T - 1):\n",
    "        edge_index.append([i, i + 1])\n",
    "        edge_index.append([i + 1, i])\n",
    "    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous().to(device)\n",
    "    \n",
    "    # Calculate stride_frames based on metadata or default\n",
    "    if stride_seconds is not None:\n",
    "        # Use stride from file metadata (e.g., 1s_1s.npz means 1 second stride)\n",
    "        stride_frames = stride_seconds * fps\n",
    "    else:\n",
    "        # Default EgoVLP config: stride=16 frames\n",
    "        stride_frames = 16\n",
    "    \n",
    "    # Create Data object with all attributes HiERO expects (following official implementation)\n",
    "    graph_data = Data(x=x, edge_index=edge_index)\n",
    "    graph_data.batch = torch.zeros(T, dtype=torch.long, device=device)\n",
    "    graph_data.pos = ((0.5 + torch.arange(T, dtype=torch.float, device=device)) * stride_frames / fps).unsqueeze(1)  # Real timestamps in seconds\n",
    "    graph_data.indices = torch.arange(T, dtype=torch.long, device=device)  # Frame indices\n",
    "    graph_data.mask = torch.ones(T, dtype=torch.bool, device=device)  # Valid frame mask\n",
    "\n",
    "    # 3. Model inference\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            output = model(graph_data)\n",
    "            \n",
    "            # HiERO returns Batch with multiple resolution graphs, take the first (highest resolution)\n",
    "            if isinstance(output, torch.Tensor):\n",
    "                hiero_features = output[:T]\n",
    "            elif hasattr(output, '__len__') and len(output) > 0:\n",
    "                # If output is a list/batch of Data objects, take the first one\n",
    "                first_output = output[0] if isinstance(output, (list, tuple)) else output\n",
    "                if isinstance(first_output, Data):\n",
    "                    hiero_features = first_output.x[:T]\n",
    "                else:\n",
    "                    hiero_features = first_output[:T] if hasattr(first_output, '__getitem__') else output.x[:T]\n",
    "            else:\n",
    "                hiero_features = output.x[:T] if hasattr(output, 'x') else output[:T]\n",
    "            \n",
    "            hiero_features = hiero_features.cpu().numpy()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ö†Ô∏è HiERO inference failed: {type(e).__name__}: {str(e)}\")\n",
    "            print(f\"     Using original features instead.\")\n",
    "            hiero_features = features\n",
    "\n",
    "    # 4. Spectral Clustering\n",
    "    features_norm = hiero_features / (np.linalg.norm(hiero_features, axis=1, keepdims=True) + 1e-8)\n",
    "    affinity = features_norm @ features_norm.T\n",
    "    \n",
    "    # Ensure affinity is valid (no NaN/Inf)\n",
    "    if np.isnan(affinity).any() or np.isinf(affinity).any():\n",
    "        print(f\"  ‚ö†Ô∏è Invalid affinity matrix, using identity\")\n",
    "        affinity = np.eye(len(features_norm))\n",
    "    \n",
    "    # Make sure affinity is symmetric and positive semi-definite\n",
    "    affinity = (affinity + affinity.T) / 2\n",
    "    affinity = np.clip(affinity, 0, 1)\n",
    "    \n",
    "    clustering = SpectralClustering(n_clusters=n_clusters, affinity='precomputed', random_state=42, assign_labels='kmeans')\n",
    "    labels = clustering.fit_predict(affinity)\n",
    "\n",
    "    # 5. Convert to boundaries (with post-processing)\n",
    "    boundaries = []\n",
    "    current_start = 0\n",
    "    for i in range(1, len(labels)):\n",
    "        if labels[i] != labels[current_start]:\n",
    "            if i - current_start >= 5: # Minimum length filtering\n",
    "                boundaries.append((current_start, i - 1))\n",
    "                current_start = i\n",
    "    if len(labels) - current_start >= 5:\n",
    "        boundaries.append((current_start, len(labels) - 1))\n",
    "    \n",
    "    # Extract Embedding\n",
    "    step_embeddings = []\n",
    "    for s, e in boundaries:\n",
    "        step_embeddings.append(hiero_features[s:e+1].mean(axis=0))\n",
    "    \n",
    "    return boundaries, np.stack(step_embeddings) if step_embeddings else np.zeros((0, 256))\n",
    "\n",
    "print(\"‚úÖ Core algorithm function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228ee5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Batch Process Videos (Main Loop) - with JSON + padded NPZ outputs\n",
    "import json\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Prepare ID list\n",
    "with open(ANNOTATIONS_PATH, 'r') as f:\n",
    "    annotations = json.load(f)\n",
    "\n",
    "available_features = {\n",
    "    f.split('_')[0] + '_' + f.split('_')[1]\n",
    "    for f in os.listdir(EGOVLP_FEATURES_DIR)\n",
    "    if f.endswith('.npz')\n",
    "}\n",
    "common_ids = list(set(annotations.keys()).intersection(available_features))\n",
    "\n",
    "print(f\"üöÄ Starting processing {len(common_ids)} videos...\")\n",
    "\n",
    "hiero_results = {}\n",
    "failed = []\n",
    "\n",
    "for vid in tqdm(common_ids):\n",
    "    try:\n",
    "        # Load features\n",
    "        f_name = [f for f in os.listdir(EGOVLP_FEATURES_DIR) if f.startswith(vid)][0]\n",
    "        data = np.load(os.path.join(EGOVLP_FEATURES_DIR, f_name))\n",
    "        feats = data['arr_0'] if 'arr_0' in data else data['features']\n",
    "\n",
    "        # Clean NaNs\n",
    "        if np.isnan(feats).any():\n",
    "            feats = np.nan_to_num(feats)\n",
    "\n",
    "        # Parse stride from filename (e.g., \"xxx_1s_1s.npz\" means stride=1s)\n",
    "        stride_match = re.search(r'_(\\d+\\.?\\d*)s_', f_name)\n",
    "        stride_seconds = float(stride_match.group(1)) if stride_match else None\n",
    "\n",
    "        # Run HiERO\n",
    "        bounds, embs = detect_steps_with_hiero(\n",
    "            feats, hiero_model, device,\n",
    "            stride_seconds=stride_seconds, fps=30\n",
    "        )\n",
    "\n",
    "        # Convert outputs to JSON/NP friendly formats\n",
    "        # embs is already numpy array from detect_steps_with_hiero\n",
    "        embs = np.asarray(embs, dtype=np.float32)\n",
    "        \n",
    "        # bounds is already a Python list of tuples [(start, end), ...]\n",
    "        # Make sure each tuple is converted to list for JSON serialization\n",
    "        bounds = [list(b) if isinstance(b, tuple) else b for b in bounds]\n",
    "\n",
    "        # Record results\n",
    "        anno = annotations[vid]\n",
    "        has_error = any(s.get('has_errors', False) for s in anno.get('steps', []))\n",
    "\n",
    "        hiero_results[vid] = {\n",
    "            'boundaries': bounds,\n",
    "            'step_embeddings': embs,  # keep raw per-video (variable length)\n",
    "            'video_label': 1 if has_error else 0,\n",
    "            'activity': anno.get('activity_name', 'unknown')\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        failed.append(vid)\n",
    "\n",
    "print(f\"\\n‚úÖ Processing complete! Success: {len(hiero_results)}, Failed: {len(failed)}\")\n",
    "\n",
    "# ---------------------------\n",
    "# A) Save raw dict NPZ (debug-friendly, keeps variable length per video)\n",
    "# ---------------------------\n",
    "raw_npz_path = os.path.join(OUTPUT_DIR, 'hiero_step_embeddings_raw.npz')\n",
    "np.savez(raw_npz_path, results=hiero_results, failed=np.array(failed, dtype=object))\n",
    "print(f\"üíæ Raw results saved to: {raw_npz_path}\")\n",
    "\n",
    "# ---------------------------\n",
    "# B) Save boundaries JSON (what you said you need)\n",
    "# ---------------------------\n",
    "bound_json = {}\n",
    "for vid, item in hiero_results.items():\n",
    "    bound_json[vid] = {\n",
    "        \"boundaries\": item[\"boundaries\"],\n",
    "        \"video_label\": item[\"video_label\"],\n",
    "        \"activity\": item[\"activity\"],\n",
    "    }\n",
    "\n",
    "json_path = os.path.join(OUTPUT_DIR, \"hiero_step_boundaries.json\")\n",
    "with open(json_path, \"w\") as f:\n",
    "    json.dump(bound_json, f, indent=2)\n",
    "print(f\"üíæ Boundaries JSON saved to: {json_path}\")\n",
    "\n",
    "# ---------------------------\n",
    "# C) Save padded embeddings NPZ (model-friendly fixed tensor + mask)\n",
    "# ---------------------------\n",
    "vids = list(hiero_results.keys())\n",
    "if len(vids) == 0:\n",
    "    raise RuntimeError(\"No successful videos to save. Check 'failed' list and upstream errors.\")\n",
    "\n",
    "# Determine emb_dim and max_steps across videos\n",
    "emb_dim = None\n",
    "max_steps = 0\n",
    "for vid in vids:\n",
    "    embs = hiero_results[vid][\"step_embeddings\"]\n",
    "    embs = np.asarray(embs, dtype=np.float32)\n",
    "    if embs.ndim == 1:\n",
    "        embs = embs.reshape(1, -1)\n",
    "    if emb_dim is None:\n",
    "        emb_dim = embs.shape[-1]\n",
    "    max_steps = max(max_steps, embs.shape[0])\n",
    "\n",
    "N = len(vids)\n",
    "hiero_emb = np.zeros((N, max_steps, emb_dim), dtype=np.float32)\n",
    "hiero_mask = np.zeros((N, max_steps), dtype=np.bool_)\n",
    "labels = np.zeros((N,), dtype=np.int64)\n",
    "\n",
    "for i, vid in enumerate(vids):\n",
    "    embs = np.asarray(hiero_results[vid][\"step_embeddings\"], dtype=np.float32)\n",
    "    if embs.ndim == 1:\n",
    "        embs = embs.reshape(1, -1)\n",
    "    k = embs.shape[0]\n",
    "    hiero_emb[i, :k, :] = embs\n",
    "    hiero_mask[i, :k] = True\n",
    "    labels[i] = int(hiero_results[vid][\"video_label\"])\n",
    "\n",
    "npz_path = os.path.join(OUTPUT_DIR, \"hiero_step_embeddings.npz\")\n",
    "np.savez(\n",
    "    npz_path,\n",
    "    video_ids=np.array(vids, dtype=object),\n",
    "    step_embeddings=hiero_emb,\n",
    "    step_mask=hiero_mask,\n",
    "    labels=labels,\n",
    ")\n",
    "print(f\"üíæ Padded embeddings NPZ saved to: {npz_path}\")\n",
    "print(f\"üìê Shapes: step_embeddings={hiero_emb.shape}, step_mask={hiero_mask.shape}, labels={labels.shape}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
