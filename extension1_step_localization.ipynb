{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b68f781f",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5cd806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository\n",
    "!git clone https://github.com/T-Larm/aml-2025-mistake-detection-gp.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67cbc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd aml-2025-mistake-detection-gp\n",
    "!git pull origin main\n",
    "!git submodule update --init --recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96c84aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install requirements\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef865ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a9774d",
   "metadata": {},
   "source": [
    "## 2. Path Configuration\n",
    "\n",
    "**‚ö†Ô∏è Modify these paths according to your Google Drive structure!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43affa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# ================= PATH CONFIGURATION =================\n",
    "# Modify these paths according to your Google Drive!\n",
    "\n",
    "# Project root (in Colab)\n",
    "PROJECT_ROOT = \"/content/aml-2025-mistake-detection-gp\"\n",
    "\n",
    "# Annotations (from the cloned repo submodule)\n",
    "# ‚ö†Ô∏è Note: The submodule path is annotations/annotation_json/complete_step_annotations.json\n",
    "ANNOTATIONS_PATH = os.path.join(PROJECT_ROOT, \"annotations/annotation_json/complete_step_annotations.json\")\n",
    "\n",
    "# Split file\n",
    "SPLIT_FILE = os.path.join(PROJECT_ROOT, \"er_annotations/recordings_combined_splits.json\")\n",
    "\n",
    "# EgoVLP features on Google Drive\n",
    "# ‚ö†Ô∏è MODIFY THIS PATH according to your Drive structure!\n",
    "EGOVLP_FEATURES_DIR = \"/content/drive/MyDrive/AMLproject/Captain_Cook_dataset/features/segments/egovlp\"\n",
    "\n",
    "# ActionFormer predictions (if using Route B)\n",
    "# Set to None if not using predicted boundaries\n",
    "ACTIONFORMER_PREDICTIONS_PATH = None  # e.g., \"/content/drive/MyDrive/.../actionformer_predictions.json\"\n",
    "\n",
    "# Output directory (save results to Drive for persistence)\n",
    "OUTPUT_DIR = \"/content/drive/MyDrive/AMLproject/extension_outputs\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(\"=== Path Configuration ===\")\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Annotations: {ANNOTATIONS_PATH}\")\n",
    "print(f\"EgoVLP features: {EGOVLP_FEATURES_DIR}\")\n",
    "print(f\"Output dir: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8c8071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify paths exist\n",
    "print(\"=== Verifying Paths ===\")\n",
    "\n",
    "# Check annotations\n",
    "if os.path.exists(ANNOTATIONS_PATH):\n",
    "    print(f\"‚úÖ Annotations file found\")\n",
    "else:\n",
    "    print(f\"‚ùå Annotations file NOT found: {ANNOTATIONS_PATH}\")\n",
    "\n",
    "# Check EgoVLP features\n",
    "if os.path.exists(EGOVLP_FEATURES_DIR):\n",
    "    files = os.listdir(EGOVLP_FEATURES_DIR)\n",
    "    npz_files = [f for f in files if f.endswith('.npz')]\n",
    "    print(f\"‚úÖ EgoVLP features found: {len(npz_files)} .npz files\")\n",
    "    print(f\"   Sample files: {npz_files[:5]}\")\n",
    "else:\n",
    "    print(f\"‚ùå EgoVLP features NOT found: {EGOVLP_FEATURES_DIR}\")\n",
    "\n",
    "# Check split file\n",
    "if os.path.exists(SPLIT_FILE):\n",
    "    print(f\"‚úÖ Split file found\")\n",
    "else:\n",
    "    print(f\"‚ùå Split file NOT found: {SPLIT_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e40c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check EgoVLP feature file structure\n",
    "import numpy as np\n",
    "\n",
    "# Find a sample file\n",
    "sample_files = [f for f in os.listdir(EGOVLP_FEATURES_DIR) if f.endswith('.npz')][:1]\n",
    "if sample_files:\n",
    "    sample_path = os.path.join(EGOVLP_FEATURES_DIR, sample_files[0])\n",
    "    data = np.load(sample_path)\n",
    "    print(f\"Sample file: {sample_files[0]}\")\n",
    "    print(f\"Keys: {list(data.keys())}\")\n",
    "    for key in data.keys():\n",
    "        print(f\"  {key}: shape = {data[key].shape}, dtype = {data[key].dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d7684f",
   "metadata": {},
   "source": [
    "## 3. Load Step Localization Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb6af1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(PROJECT_ROOT)\n",
    "\n",
    "from extension.step_localization import (\n",
    "    StepLocalizer,\n",
    "    PredictedBoundaryLocalizer,\n",
    "    prepare_dataset_for_task_verification,\n",
    "    compare_gt_vs_predicted\n",
    ")\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "print(\"‚úÖ Step localization module loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d30328",
   "metadata": {},
   "source": [
    "## 4. Route A: Ground Truth Boundaries\n",
    "\n",
    "This is the **upper bound** baseline. Using perfect step boundaries from annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ef68f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize GT Localizer\n",
    "gt_localizer = StepLocalizer(\n",
    "    annotations_path=ANNOTATIONS_PATH,\n",
    "    features_dir=EGOVLP_FEATURES_DIR,\n",
    "    fps=1.0,  # EgoVLP features are extracted at 1 FPS\n",
    "    feature_key='arr_0'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10342fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a single video\n",
    "# Find a video that has both annotations and features\n",
    "with open(ANNOTATIONS_PATH, 'r') as f:\n",
    "    annotations = json.load(f)\n",
    "\n",
    "# Get list of available feature files\n",
    "available_features = set()\n",
    "for f in os.listdir(EGOVLP_FEATURES_DIR):\n",
    "    if f.endswith('.npz'):\n",
    "        # Extract recording_id from filename: \"9_8_360p_224.mp4_1s_1s.npz\" -> \"9_8\"\n",
    "        recording_id = '_'.join(f.split('_')[:2])\n",
    "        available_features.add(recording_id)\n",
    "\n",
    "# Find videos with both annotations and features\n",
    "annotated_ids = set(annotations.keys())\n",
    "common_ids = annotated_ids.intersection(available_features)\n",
    "print(f\"Videos with both annotations and features: {len(common_ids)}\")\n",
    "print(f\"Sample IDs: {list(common_ids)[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a4a95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process a single video\n",
    "test_id = list(common_ids)[0]\n",
    "print(f\"\\n=== Processing video: {test_id} ===\")\n",
    "\n",
    "video_data = gt_localizer.process_video(test_id)\n",
    "\n",
    "if video_data:\n",
    "    print(f\"\\nVideo: {video_data.recording_id}\")\n",
    "    print(f\"Activity: {video_data.activity_name}\")\n",
    "    print(f\"Number of steps: {len(video_data.steps)}\")\n",
    "    print(f\"Video label (0=correct, 1=has errors): {video_data.video_label}\")\n",
    "    \n",
    "    print(\"\\nSteps:\")\n",
    "    for i, step in enumerate(video_data.steps):\n",
    "        error_str = \"‚ùå ERROR\" if step.has_errors else \"‚úì\"\n",
    "        print(f\"  [{i+1}] Step {step.step_id}: {step.start_time:.1f}s - {step.end_time:.1f}s {error_str}\")\n",
    "        print(f\"       {step.description[:60]}...\")\n",
    "        print(f\"       Embedding shape: {step.embedding.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b223c5",
   "metadata": {},
   "source": [
    "### 4.1 Process All Available Videos (Route A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2f9311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all videos that have features\n",
    "print(f\"Processing {len(common_ids)} videos with GT boundaries...\")\n",
    "\n",
    "gt_results = gt_localizer.process_all_videos(list(common_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573ac54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistics\n",
    "num_steps_list = [len(v.steps) for v in gt_results.values()]\n",
    "labels = [v.video_label for v in gt_results.values()]\n",
    "\n",
    "print(\"\\n=== Route A Statistics (GT Boundaries) ===\")\n",
    "print(f\"Total videos processed: {len(gt_results)}\")\n",
    "print(f\"Videos with errors: {sum(labels)} ({sum(labels)/len(labels)*100:.1f}%)\")\n",
    "print(f\"Videos without errors: {len(labels) - sum(labels)} ({(len(labels)-sum(labels))/len(labels)*100:.1f}%)\")\n",
    "print(f\"Avg steps per video: {np.mean(num_steps_list):.1f}\")\n",
    "print(f\"Min/Max steps: {min(num_steps_list)} / {max(num_steps_list)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25790c89",
   "metadata": {},
   "source": [
    "### 4.2 Prepare Dataset for Substep 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cda628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the max steps for padding\n",
    "max_steps = max(len(vd.steps) for vd in gt_results.values())\n",
    "print(f\"Max steps in dataset: {max_steps}\")\n",
    "\n",
    "# Prepare data arrays\n",
    "all_embeddings = []\n",
    "all_labels = []\n",
    "all_masks = []\n",
    "all_ids = []\n",
    "\n",
    "for recording_id, video_data in gt_results.items():\n",
    "    embeddings, mask, _ = gt_localizer.get_step_embeddings_matrix(\n",
    "        video_data,\n",
    "        pad_to_length=max_steps\n",
    "    )\n",
    "    all_embeddings.append(embeddings)\n",
    "    all_labels.append(video_data.video_label)\n",
    "    all_masks.append(mask)\n",
    "    all_ids.append(recording_id)\n",
    "\n",
    "# Stack into arrays\n",
    "gt_dataset = {\n",
    "    'embeddings': np.stack(all_embeddings, axis=0),  # (N, max_steps, 256)\n",
    "    'labels': np.array(all_labels),                   # (N,)\n",
    "    'masks': np.stack(all_masks, axis=0),             # (N, max_steps)\n",
    "    'recording_ids': all_ids,\n",
    "    'max_steps': max_steps\n",
    "}\n",
    "\n",
    "print(f\"\\n=== Dataset Ready for Substep 2 ===\")\n",
    "print(f\"Embeddings shape: {gt_dataset['embeddings'].shape}\")\n",
    "print(f\"Labels shape: {gt_dataset['labels'].shape}\")\n",
    "print(f\"Masks shape: {gt_dataset['masks'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc99e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataset to Google Drive\n",
    "output_path = os.path.join(OUTPUT_DIR, \"gt_step_embeddings.npz\")\n",
    "np.savez(\n",
    "    output_path,\n",
    "    embeddings=gt_dataset['embeddings'],\n",
    "    labels=gt_dataset['labels'],\n",
    "    masks=gt_dataset['masks'],\n",
    "    recording_ids=np.array(gt_dataset['recording_ids'], dtype=object),\n",
    "    max_steps=gt_dataset['max_steps']\n",
    ")\n",
    "print(f\"‚úÖ Dataset saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f29d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_HIERO:\n",
    "    # Compare GT vs HiERO detected steps\n",
    "    comparison = []\n",
    "    \n",
    "    for recording_id in common_ids:\n",
    "        if recording_id not in hiero_results:\n",
    "            continue\n",
    "        \n",
    "        gt_video = gt_results.get(recording_id)\n",
    "        hiero_data = hiero_results.get(recording_id)\n",
    "        \n",
    "        if gt_video and hiero_data:\n",
    "            gt_num_steps = len(gt_video.steps)\n",
    "            hiero_num_steps = len(hiero_data['boundaries'])\n",
    "            \n",
    "            comparison.append({\n",
    "                'recording_id': recording_id,\n",
    "                'gt_steps': gt_num_steps,\n",
    "                'hiero_steps': hiero_num_steps,\n",
    "                'difference': hiero_num_steps - gt_num_steps,\n",
    "                'video_label': hiero_data['video_label']\n",
    "            })\n",
    "    \n",
    "    # Statistics\n",
    "    differences = [c['difference'] for c in comparison]\n",
    "    print(f\"\\n=== GT vs HiERO Comparison ===\")\n",
    "    print(f\"Videos compared: {len(comparison)}\")\n",
    "    print(f\"Average difference (HiERO - GT): {np.mean(differences):.2f} steps\")\n",
    "    print(f\"Std dev: {np.std(differences):.2f}\")\n",
    "    print(f\"Min/Max difference: {min(differences)} / {max(differences)}\")\n",
    "    \n",
    "    # Save comparison\n",
    "    comparison_path = os.path.join(OUTPUT_DIR, \"gt_vs_hiero_comparison.json\")\n",
    "    with open(comparison_path, 'w') as f:\n",
    "        json.dump(comparison, f, indent=2)\n",
    "    print(f\"‚úÖ Comparison saved to: {comparison_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e68b070",
   "metadata": {},
   "source": [
    "### 5.7 Compare GT vs HiERO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e77e7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_HIERO:\n",
    "    # Save HiERO dataset\n",
    "    hiero_output_path = os.path.join(OUTPUT_DIR, \"hiero_step_embeddings.npz\")\n",
    "    np.savez(\n",
    "        hiero_output_path,\n",
    "        embeddings=hiero_dataset['embeddings'],\n",
    "        labels=hiero_dataset['labels'],\n",
    "        masks=hiero_dataset['masks'],\n",
    "        recording_ids=np.array(hiero_dataset['recording_ids'], dtype=object),\n",
    "        max_steps=hiero_dataset['max_steps']\n",
    "    )\n",
    "    print(f\"‚úÖ HiERO dataset saved to: {hiero_output_path}\")\n",
    "    \n",
    "    # Also save boundaries as JSON\n",
    "    boundaries_json = {\n",
    "        rec_id: {\n",
    "            'boundaries': [(int(s), int(e)) for s, e in data['boundaries']],\n",
    "            'num_steps': len(data['boundaries']),\n",
    "            'video_label': int(data['video_label']),\n",
    "            'activity': data['activity_name']\n",
    "        }\n",
    "        for rec_id, data in hiero_results.items()\n",
    "    }\n",
    "    \n",
    "    boundaries_path = os.path.join(OUTPUT_DIR, \"hiero_step_boundaries.json\")\n",
    "    with open(boundaries_path, 'w') as f:\n",
    "        json.dump(boundaries_json, f, indent=2)\n",
    "    print(f\"‚úÖ HiERO boundaries saved to: {boundaries_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eae61fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_HIERO:\n",
    "    # Prepare padded dataset\n",
    "    max_steps_hiero = max(len(v['boundaries']) for v in hiero_results.values())\n",
    "    print(f\"Max steps in HiERO dataset: {max_steps_hiero}\")\n",
    "    \n",
    "    hiero_embeddings = []\n",
    "    hiero_masks = []\n",
    "    hiero_labels = []\n",
    "    hiero_ids = []\n",
    "    \n",
    "    for recording_id, data in hiero_results.items():\n",
    "        step_emb = data['step_embeddings']  # (num_steps, 256)\n",
    "        num_steps = len(step_emb)\n",
    "        \n",
    "        # Pad embeddings\n",
    "        padded_emb = np.zeros((max_steps_hiero, 256), dtype=np.float32)\n",
    "        mask = np.zeros(max_steps_hiero, dtype=bool)\n",
    "        \n",
    "        padded_emb[:num_steps] = step_emb\n",
    "        mask[:num_steps] = True\n",
    "        \n",
    "        hiero_embeddings.append(padded_emb)\n",
    "        hiero_masks.append(mask)\n",
    "        hiero_labels.append(data['video_label'])\n",
    "        hiero_ids.append(recording_id)\n",
    "    \n",
    "    # Stack into arrays\n",
    "    hiero_dataset = {\n",
    "        'embeddings': np.stack(hiero_embeddings, axis=0),  # (N, max_steps, 256)\n",
    "        'labels': np.array(hiero_labels),                   # (N,)\n",
    "        'masks': np.stack(hiero_masks, axis=0),             # (N, max_steps)\n",
    "        'recording_ids': hiero_ids,\n",
    "        'max_steps': max_steps_hiero\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n=== HiERO Dataset Ready for Substep 2 ===\")\n",
    "    print(f\"Embeddings shape: {hiero_dataset['embeddings'].shape}\")\n",
    "    print(f\"Labels shape: {hiero_dataset['labels'].shape}\")\n",
    "    print(f\"Masks shape: {hiero_dataset['masks'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41fe818",
   "metadata": {},
   "source": [
    "### 5.6 Prepare HiERO Dataset for Substep 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f033176b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_HIERO:\n",
    "    # Statistics\n",
    "    hiero_num_steps = [len(v['boundaries']) for v in hiero_results.values()]\n",
    "    hiero_labels = [v['video_label'] for v in hiero_results.values()]\n",
    "    \n",
    "    print(\"\\n=== Route B Statistics (HiERO Boundaries) ===\")\n",
    "    print(f\"Total videos processed: {len(hiero_results)}\")\n",
    "    print(f\"Videos with errors: {sum(hiero_labels)} ({sum(hiero_labels)/len(hiero_labels)*100:.1f}%)\")\n",
    "    print(f\"Videos without errors: {len(hiero_labels) - sum(hiero_labels)}\")\n",
    "    print(f\"Avg steps per video: {np.mean(hiero_num_steps):.1f}\")\n",
    "    print(f\"Min/Max steps: {min(hiero_num_steps)} / {max(hiero_num_steps)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6983fc33",
   "metadata": {},
   "source": [
    "### 5.5 HiERO Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed214cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_HIERO:\n",
    "    from tqdm import tqdm\n",
    "    \n",
    "    # Process all videos with HiERO\n",
    "    hiero_results = {}\n",
    "    \n",
    "    print(f\"Processing {len(common_ids)} videos with HiERO...\")\n",
    "    \n",
    "    for recording_id in tqdm(list(common_ids)):\n",
    "        try:\n",
    "            # Find feature file\n",
    "            feature_files = [f for f in os.listdir(EGOVLP_FEATURES_DIR) \n",
    "                           if f.startswith(recording_id.replace('_', '_')) and f.endswith('.npz')]\n",
    "            \n",
    "            if not feature_files:\n",
    "                continue\n",
    "            \n",
    "            # Load features\n",
    "            feature_path = os.path.join(EGOVLP_FEATURES_DIR, feature_files[0])\n",
    "            data = np.load(feature_path)\n",
    "            features = data['arr_0']\n",
    "            \n",
    "            # Detect steps with HiERO\n",
    "            boundaries, step_embeddings = detect_steps_with_hiero(\n",
    "                features, hiero_model, device\n",
    "            )\n",
    "            \n",
    "            # Get video label from annotations\n",
    "            anno = annotations[recording_id]\n",
    "            has_errors = any(step.get('has_errors', False) for step in anno.get('steps', []))\n",
    "            video_label = 1 if has_errors else 0\n",
    "            \n",
    "            # Store results\n",
    "            hiero_results[recording_id] = {\n",
    "                'boundaries': boundaries,\n",
    "                'step_embeddings': step_embeddings,\n",
    "                'video_label': video_label,\n",
    "                'activity_name': anno.get('activity_name', 'unknown')\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {recording_id}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\n‚úÖ Processed {len(hiero_results)} videos with HiERO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c429c9ef",
   "metadata": {},
   "source": [
    "### 5.4 Process Videos with HiERO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14c36cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_HIERO:\n",
    "    from sklearn.cluster import KMeans\n",
    "    \n",
    "    def detect_steps_with_hiero(features, model, device, n_clusters='auto'):\n",
    "        \"\"\"\n",
    "        Detect step boundaries using HiERO model.\n",
    "        \n",
    "        Args:\n",
    "            features: (T, 256) numpy array\n",
    "            model: HiERO model\n",
    "            device: torch device\n",
    "            n_clusters: number of clusters ('auto' or int)\n",
    "        \n",
    "        Returns:\n",
    "            boundaries: list of (start, end) tuples\n",
    "            step_embeddings: (num_steps, 256) array\n",
    "        \"\"\"\n",
    "        T, D = features.shape\n",
    "        \n",
    "        # Auto-estimate clusters\n",
    "        if n_clusters == 'auto':\n",
    "            n_clusters = max(2, min(T // 30, 15))\n",
    "        \n",
    "        # Convert to torch\n",
    "        x = torch.from_numpy(features).float().to(device)\n",
    "        \n",
    "        # Create temporal graph\n",
    "        edge_index = []\n",
    "        for i in range(T - 1):\n",
    "            edge_index.append([i, i + 1])\n",
    "            edge_index.append([i + 1, i])\n",
    "        edge_index = torch.tensor(edge_index, dtype=torch.long).t().to(device)\n",
    "        \n",
    "        # Create graph data\n",
    "        graph_data = Data(x=x, edge_index=edge_index)\n",
    "        batch = Batch.from_data_list([graph_data])\n",
    "        \n",
    "        # HiERO forward pass\n",
    "        with torch.no_grad():\n",
    "            try:\n",
    "                output = hiero_model(batch)\n",
    "                if isinstance(output, dict):\n",
    "                    hiero_features = output.get('features', output.get('x', x))\n",
    "                else:\n",
    "                    hiero_features = output\n",
    "                hiero_features = hiero_features.cpu().numpy()\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è HiERO failed, using original features: {e}\")\n",
    "                hiero_features = features\n",
    "        \n",
    "        # Clustering on HiERO features\n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "        labels = kmeans.fit_predict(hiero_features[:T])\n",
    "        \n",
    "        # Find boundaries\n",
    "        boundaries = []\n",
    "        current_start = 0\n",
    "        current_label = labels[0]\n",
    "        \n",
    "        for i in range(1, len(labels)):\n",
    "            if labels[i] != current_label:\n",
    "                if i - current_start >= 5:  # Min 5 frames\n",
    "                    boundaries.append((current_start, i - 1))\n",
    "                    current_start = i\n",
    "                    current_label = labels[i]\n",
    "        \n",
    "        # Add last segment\n",
    "        if len(labels) - current_start >= 5:\n",
    "            boundaries.append((current_start, len(labels) - 1))\n",
    "        \n",
    "        # Extract step embeddings\n",
    "        step_embeddings = []\n",
    "        for start, end in boundaries:\n",
    "            step_emb = hiero_features[start:end+1].mean(axis=0)\n",
    "            step_embeddings.append(step_emb)\n",
    "        \n",
    "        step_embeddings = np.stack(step_embeddings, axis=0) if step_embeddings else np.zeros((0, 256))\n",
    "        \n",
    "        return boundaries, step_embeddings\n",
    "    \n",
    "    print(\"‚úÖ HiERO detection function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f46f64",
   "metadata": {},
   "source": [
    "### 5.3 HiERO Step Detection Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba13dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_HIERO:\n",
    "    import sys\n",
    "    sys.path.insert(0, '/content/HiERO')\n",
    "    \n",
    "    import torch\n",
    "    import yaml\n",
    "    from models.hiero import HiERO\n",
    "    from torch_geometric.data import Data, Batch\n",
    "    \n",
    "    # Check device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Load checkpoint\n",
    "    if os.path.exists(HIERO_CHECKPOINT):\n",
    "        checkpoint = torch.load(HIERO_CHECKPOINT, map_location='cpu')\n",
    "        print(f\"‚úÖ Checkpoint loaded ({os.path.getsize(HIERO_CHECKPOINT)/(1024*1024):.1f} MB)\")\n",
    "        \n",
    "        # Load config\n",
    "        if 'config' in checkpoint:\n",
    "            config = checkpoint['config']\n",
    "        else:\n",
    "            config_path = '/content/HiERO/configs/egovlp.yaml'\n",
    "            with open(config_path, 'r') as f:\n",
    "                config = yaml.safe_load(f)\n",
    "        \n",
    "        # Initialize model\n",
    "        model_config = config.get('model', {})\n",
    "        hiero_model = HiERO(input_size=256, **model_config)\n",
    "        \n",
    "        # Load weights\n",
    "        state_dict = checkpoint.get('state_dict', checkpoint.get('model', checkpoint))\n",
    "        try:\n",
    "            hiero_model.load_state_dict(state_dict, strict=True)\n",
    "            print(\"‚úÖ Model weights loaded (strict)\")\n",
    "        except:\n",
    "            hiero_model.load_state_dict(state_dict, strict=False)\n",
    "            print(\"‚ö†Ô∏è Model weights loaded (non-strict)\")\n",
    "        \n",
    "        hiero_model = hiero_model.to(device)\n",
    "        hiero_model.eval()\n",
    "        print(f\"‚úÖ HiERO model ready ({sum(p.numel() for p in hiero_model.parameters())/1e6:.1f}M params)\")\n",
    "    else:\n",
    "        print(f\"‚ùå Checkpoint not found: {HIERO_CHECKPOINT}\")\n",
    "        USE_HIERO = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62816562",
   "metadata": {},
   "source": [
    "### 5.2 Load HiERO Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1354f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_HIERO:\n",
    "    # Clone HiERO repository\n",
    "    import os\n",
    "    if not os.path.exists('/content/HiERO'):\n",
    "        !git clone https://github.com/sapeirone/HiERO.git /content/HiERO\n",
    "        print(\"‚úÖ HiERO repository cloned\")\n",
    "    else:\n",
    "        print(\"‚úÖ HiERO repository already exists\")\n",
    "    \n",
    "    # Install HiERO dependencies\n",
    "    %cd /content/HiERO\n",
    "    !pip install -q torch-geometric torch-scatter torch-sparse -f https://data.pyg.org/whl/torch-2.4.0+cu124.html\n",
    "    !pip install -q einops torch_kmeans tqdm\n",
    "    \n",
    "    # Return to project directory\n",
    "    %cd {PROJECT_ROOT}\n",
    "    print(\"‚úÖ HiERO dependencies installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b643e1",
   "metadata": {},
   "source": [
    "### 5.1 Setup HiERO Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b276a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HiERO model configuration\n",
    "USE_HIERO = True  # Set to False to skip HiERO route\n",
    "\n",
    "# HiERO checkpoint path - MODIFY THIS!\n",
    "HIERO_CHECKPOINT = \"/content/drive/MyDrive/AMLproject/HiERO_for_egovlp/hiero_egovlp/hiero_egovlp.pth\"\n",
    "\n",
    "if USE_HIERO:\n",
    "    print(\"‚úÖ HiERO route enabled\")\n",
    "    print(f\"Checkpoint: {HIERO_CHECKPOINT}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Skipping HiERO route\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5af03b",
   "metadata": {},
   "source": [
    "## 5. Route B: HiERO Model-based Boundaries\n",
    "\n",
    "This evaluates the **end-to-end system** using step boundaries predicted by HiERO model.\n",
    "\n",
    "Uses hierarchical clustering from HiERO for better step detection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06564690",
   "metadata": {},
   "source": [
    "## 6. Summary & Next Steps\n",
    "\n",
    "### What we have now:\n",
    "1. **GT Step Embeddings** (`gt_step_embeddings.npz`) - Upper bound baseline\n",
    "   - Shape: `(N, max_steps, 256)`\n",
    "   - Uses perfect step boundaries from annotations\n",
    "   \n",
    "2. **HiERO Step Embeddings** (`hiero_step_embeddings.npz`) - Predicted boundaries\n",
    "   - Shape: `(N, max_steps, 256)`\n",
    "   - Uses HiERO model for step detection\n",
    "   - More realistic end-to-end system performance\n",
    "\n",
    "3. **HiERO Boundaries** (`hiero_step_boundaries.json`)\n",
    "   - Predicted step boundaries for each video\n",
    "   - Ready for Substeps 3 & 4 (Task Graph matching)\n",
    "\n",
    "### Next Steps:\n",
    "1. **Substep 2**: Train Transformer classifier on step embeddings\n",
    "   - Test with both GT (upper bound) and HiERO (realistic) embeddings\n",
    "2. **Substep 3**: Encode task graph nodes, match with HiERO visual features\n",
    "3. **Substep 4**: Train GNN classifier on matched task graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25f8bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "print(\"=\"*60)\n",
    "print(\"Extension Substep 1 Complete!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nüìä Results saved to: {OUTPUT_DIR}\")\n",
    "print(f\"\\nüìÅ Files created:\")\n",
    "for f in os.listdir(OUTPUT_DIR):\n",
    "    fpath = os.path.join(OUTPUT_DIR, f)\n",
    "    size = os.path.getsize(fpath) / (1024*1024)  # MB\n",
    "    print(f\"   - {f} ({size:.2f} MB)\")\n",
    "\n",
    "print(f\"\\nüéØ Summary:\")\n",
    "print(f\"   - GT baseline: {len(gt_results)} videos (upper bound)\")\n",
    "if USE_HIERO:\n",
    "    print(f\"   - HiERO predictions: {len(hiero_results)} videos (realistic)\")\n",
    "    print(f\"   - Average steps - GT: {np.mean(num_steps_list):.1f}, HiERO: {np.mean(hiero_num_steps):.1f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
