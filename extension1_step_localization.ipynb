{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b68f781f",
   "metadata": {
    "id": "b68f781f"
   },
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6h_CAuIZu46d",
   "metadata": {
    "id": "6h_CAuIZu46d"
   },
   "source": [
    "## Install Python 3.10\n",
    "\n",
    "### Subtask:\n",
    "Install Python 3.10 and its necessary dependencies using apt-get.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9Ka51wvku9ab",
   "metadata": {
    "id": "9Ka51wvku9ab"
   },
   "outputs": [],
   "source": [
    "print(\"Installing Python 3.10...\")\n",
    "!sudo apt-get update -y\n",
    "!apt-get install python3.10 python3.10-dev python3.10-venv -y\n",
    "print(\"Python 3.10 installation complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rX0O7DAEvA3x",
   "metadata": {
    "id": "rX0O7DAEvA3x"
   },
   "outputs": [],
   "source": [
    "# Update alternatives to point 'python3' to 'python3.10'\n",
    "# This command adds python3.10 as an alternative for python3 with a priority of 1.\n",
    "!update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.10 1\n",
    "\n",
    "# This command then explicitly sets python3 to use python3.10\n",
    "!update-alternatives --set python3 /usr/bin/python3.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "IwwwTKDXvEqR",
   "metadata": {
    "id": "IwwwTKDXvEqR"
   },
   "outputs": [],
   "source": [
    "# Install pip for Python 3 (now linked to Python 3.10)\n",
    "# Using apt-get is the recommended way for system-wide pip in Debian/Ubuntu\n",
    "!apt-get install python3-pip -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ONPweGuPvG2P",
   "metadata": {
    "id": "ONPweGuPvG2P"
   },
   "outputs": [],
   "source": [
    "# Install pip for Python 3.10\n",
    "# Use ensurepip to guarantee pip is installed for the new python version\n",
    "!python3 -m ensurepip --upgrade\n",
    "# Upgrade pip, setuptools, and wheel for the new Python environment\n",
    "!python3 -m pip install --upgrade pip setuptools wheel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2xEJHB-6vJNa",
   "metadata": {
    "id": "2xEJHB-6vJNa"
   },
   "outputs": [],
   "source": [
    "# Verify the Python version after switching\n",
    "!python3 --version\n",
    "# Python 3.10.12\n",
    "# After verifying, please remember to restart the Colab runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5cd806",
   "metadata": {
    "id": "cb5cd806"
   },
   "outputs": [],
   "source": [
    "# Clone the repository\n",
    "!git clone https://github.com/T-Larm/aml-2025-mistake-detection-gp.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67cbc26",
   "metadata": {
    "id": "e67cbc26"
   },
   "outputs": [],
   "source": [
    "%cd aml-2025-mistake-detection-gp\n",
    "!git pull origin main\n",
    "!git submodule update --init --recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96c84aa",
   "metadata": {
    "id": "f96c84aa"
   },
   "outputs": [],
   "source": [
    "# Install requirements\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef865ac",
   "metadata": {
    "id": "8ef865ac"
   },
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a9774d",
   "metadata": {
    "id": "24a9774d"
   },
   "source": [
    "## 2. Path Configuration\n",
    "\n",
    "**‚ö†Ô∏è Modify these paths according to your Google Drive structure!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43affa7",
   "metadata": {
    "id": "a43affa7"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# ================= PATH CONFIGURATION =================\n",
    "# Modify these paths according to your Google Drive!\n",
    "\n",
    "# Project root (in Colab)\n",
    "PROJECT_ROOT = \"/content/aml-2025-mistake-detection-gp\"\n",
    "\n",
    "# Annotations (from the cloned repo submodule)\n",
    "# ‚ö†Ô∏è Note: The submodule path is annotations/annotation_json/complete_step_annotations.json\n",
    "ANNOTATIONS_PATH = os.path.join(PROJECT_ROOT, \"annotations/annotation_json/complete_step_annotations.json\")\n",
    "\n",
    "# Split file\n",
    "SPLIT_FILE = os.path.join(PROJECT_ROOT, \"er_annotations/recordings_combined_splits.json\")\n",
    "\n",
    "# EgoVLP features on Google Drive\n",
    "# ‚ö†Ô∏è MODIFY THIS PATH according to your Drive structure!\n",
    "EGOVLP_FEATURES_DIR = \"/content/drive/MyDrive/AMLproject/our_features/gopro/segments/egovlp\"\n",
    "\n",
    "# Output directory (save results to Drive for persistence)\n",
    "OUTPUT_DIR = \"/content/drive/MyDrive/AMLproject/extension1_outputs\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(\"=== Path Configuration ===\")\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Annotations: {ANNOTATIONS_PATH}\")\n",
    "print(f\"EgoVLP features: {EGOVLP_FEATURES_DIR}\")\n",
    "print(f\"Output dir: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8c8071",
   "metadata": {
    "id": "3e8c8071"
   },
   "outputs": [],
   "source": [
    "# Verify paths exist\n",
    "print(\"=== Verifying Paths ===\")\n",
    "\n",
    "# Check annotations\n",
    "if os.path.exists(ANNOTATIONS_PATH):\n",
    "    print(f\"‚úÖ Annotations file found\")\n",
    "else:\n",
    "    print(f\"‚ùå Annotations file NOT found: {ANNOTATIONS_PATH}\")\n",
    "\n",
    "# Check EgoVLP features\n",
    "if os.path.exists(EGOVLP_FEATURES_DIR):\n",
    "    files = os.listdir(EGOVLP_FEATURES_DIR)\n",
    "    npz_files = [f for f in files if f.endswith('.npz')]\n",
    "    print(f\"‚úÖ EgoVLP features found: {len(npz_files)} .npz files\")\n",
    "    print(f\"   Sample files: {npz_files[:5]}\")\n",
    "else:\n",
    "    print(f\"‚ùå EgoVLP features NOT found: {EGOVLP_FEATURES_DIR}\")\n",
    "\n",
    "# Check split file\n",
    "if os.path.exists(SPLIT_FILE):\n",
    "    print(f\"‚úÖ Split file found\")\n",
    "else:\n",
    "    print(f\"‚ùå Split file NOT found: {SPLIT_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e40c11",
   "metadata": {
    "id": "d6e40c11"
   },
   "outputs": [],
   "source": [
    "# Check EgoVLP feature file structure\n",
    "import numpy as np\n",
    "\n",
    "# Find a sample file\n",
    "sample_files = [f for f in os.listdir(EGOVLP_FEATURES_DIR) if f.endswith('.npz')][:1]\n",
    "if sample_files:\n",
    "    sample_path = os.path.join(EGOVLP_FEATURES_DIR, sample_files[0])\n",
    "    data = np.load(sample_path)\n",
    "    print(f\"Sample file: {sample_files[0]}\")\n",
    "    print(f\"Keys: {list(data.keys())}\")\n",
    "    for key in data.keys():\n",
    "        print(f\"  {key}: shape = {data[key].shape}, dtype = {data[key].dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d7684f",
   "metadata": {
    "id": "03d7684f"
   },
   "source": [
    "## 3. Load Step Localization Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb6af1e",
   "metadata": {
    "id": "bdb6af1e"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(PROJECT_ROOT)\n",
    "\n",
    "from extension.step_localization import (\n",
    "    StepLocalizer,\n",
    "    PredictedBoundaryLocalizer,\n",
    "    prepare_dataset_for_task_verification,\n",
    "    compare_gt_vs_predicted\n",
    ")\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "print(\"‚úÖ Step localization module loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d30328",
   "metadata": {
    "id": "77d30328"
   },
   "source": [
    "## 4. Route A: Ground Truth Boundaries\n",
    "\n",
    "This is the **upper bound** baseline. Using perfect step boundaries from annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ef68f3",
   "metadata": {
    "id": "e4ef68f3"
   },
   "outputs": [],
   "source": [
    "# Initialize GT Localizer\n",
    "gt_localizer = StepLocalizer(\n",
    "    annotations_path=ANNOTATIONS_PATH,\n",
    "    features_dir=EGOVLP_FEATURES_DIR,\n",
    "    fps=1.0,  # EgoVLP features are extracted at 1 FPS\n",
    "    feature_key='arr_0'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10342fb4",
   "metadata": {
    "id": "10342fb4"
   },
   "outputs": [],
   "source": [
    "# Test with a single video\n",
    "# Find a video that has both annotations and features\n",
    "with open(ANNOTATIONS_PATH, 'r') as f:\n",
    "    annotations = json.load(f)\n",
    "\n",
    "# Get list of available feature files\n",
    "available_features = set()\n",
    "for f in os.listdir(EGOVLP_FEATURES_DIR):\n",
    "    if f.endswith('.npz'):\n",
    "        # Extract recording_id from filename: \"9_8_360p_224.mp4_1s_1s.npz\" -> \"9_8\"\n",
    "        recording_id = '_'.join(f.split('_')[:2])\n",
    "        available_features.add(recording_id)\n",
    "\n",
    "# Find videos with both annotations and features\n",
    "annotated_ids = set(annotations.keys())\n",
    "common_ids = annotated_ids.intersection(available_features)\n",
    "print(f\"Videos with both annotations and features: {len(common_ids)}\")\n",
    "print(f\"Sample IDs: {list(common_ids)[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a4a95c",
   "metadata": {
    "id": "f7a4a95c"
   },
   "outputs": [],
   "source": [
    "# Process a single video\n",
    "test_id = list(common_ids)[0]\n",
    "print(f\"\\n=== Processing video: {test_id} ===\")\n",
    "\n",
    "video_data = gt_localizer.process_video(test_id)\n",
    "\n",
    "if video_data:\n",
    "    print(f\"\\nVideo: {video_data.recording_id}\")\n",
    "    print(f\"Activity: {video_data.activity_name}\")\n",
    "    print(f\"Number of steps: {len(video_data.steps)}\")\n",
    "    print(f\"Video label (0=correct, 1=has errors): {video_data.video_label}\")\n",
    "\n",
    "    print(\"\\nSteps:\")\n",
    "    for i, step in enumerate(video_data.steps):\n",
    "        error_str = \"‚ùå ERROR\" if step.has_errors else \"‚úì\"\n",
    "        print(f\"  [{i+1}] Step {step.step_id}: {step.start_time:.1f}s - {step.end_time:.1f}s {error_str}\")\n",
    "        print(f\"       {step.description[:60]}...\")\n",
    "        print(f\"       Embedding shape: {step.embedding.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b223c5",
   "metadata": {
    "id": "85b223c5"
   },
   "source": [
    "### 4.1 Process All Available Videos (Route A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2f9311",
   "metadata": {
    "id": "8a2f9311"
   },
   "outputs": [],
   "source": [
    "# Process all videos that have features\n",
    "print(f\"Processing {len(common_ids)} videos with GT boundaries...\")\n",
    "\n",
    "gt_results = gt_localizer.process_all_videos(list(common_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573ac54a",
   "metadata": {
    "id": "573ac54a"
   },
   "outputs": [],
   "source": [
    "# Statistics\n",
    "num_steps_list = [len(v.steps) for v in gt_results.values()]\n",
    "labels = [v.video_label for v in gt_results.values()]\n",
    "\n",
    "print(\"\\n=== Route A Statistics (GT Boundaries) ===\")\n",
    "print(f\"Total videos processed: {len(gt_results)}\")\n",
    "print(f\"Videos with errors: {sum(labels)} ({sum(labels)/len(labels)*100:.1f}%)\")\n",
    "print(f\"Videos without errors: {len(labels) - sum(labels)} ({(len(labels)-sum(labels))/len(labels)*100:.1f}%)\")\n",
    "print(f\"Avg steps per video: {np.mean(num_steps_list):.1f}\")\n",
    "print(f\"Min/Max steps: {min(num_steps_list)} / {max(num_steps_list)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25790c89",
   "metadata": {
    "id": "25790c89"
   },
   "source": [
    "### 4.2 Prepare Dataset for Substep 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cda628",
   "metadata": {
    "id": "39cda628"
   },
   "outputs": [],
   "source": [
    "# Get the max steps for padding\n",
    "max_steps = max(len(vd.steps) for vd in gt_results.values())\n",
    "print(f\"Max steps in dataset: {max_steps}\")\n",
    "\n",
    "# Prepare data arrays\n",
    "all_embeddings = []\n",
    "all_labels = []\n",
    "all_masks = []\n",
    "all_ids = []\n",
    "\n",
    "for recording_id, video_data in gt_results.items():\n",
    "    embeddings, mask, _ = gt_localizer.get_step_embeddings_matrix(\n",
    "        video_data,\n",
    "        pad_to_length=max_steps\n",
    "    )\n",
    "    all_embeddings.append(embeddings)\n",
    "    all_labels.append(video_data.video_label)\n",
    "    all_masks.append(mask)\n",
    "    all_ids.append(recording_id)\n",
    "\n",
    "# Stack into arrays\n",
    "gt_dataset = {\n",
    "    'embeddings': np.stack(all_embeddings, axis=0),  # (N, max_steps, 256)\n",
    "    'labels': np.array(all_labels),                   # (N,)\n",
    "    'masks': np.stack(all_masks, axis=0),             # (N, max_steps)\n",
    "    'recording_ids': all_ids,\n",
    "    'max_steps': max_steps\n",
    "}\n",
    "\n",
    "print(f\"\\n=== Dataset Ready for Substep 2 ===\")\n",
    "print(f\"Embeddings shape: {gt_dataset['embeddings'].shape}\")\n",
    "print(f\"Labels shape: {gt_dataset['labels'].shape}\")\n",
    "print(f\"Masks shape: {gt_dataset['masks'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc99e65",
   "metadata": {
    "id": "3bc99e65"
   },
   "outputs": [],
   "source": [
    "# Save dataset to Google Drive\n",
    "output_path = os.path.join(OUTPUT_DIR, \"gt_step_embeddings.npz\")\n",
    "np.savez(\n",
    "    output_path,\n",
    "    embeddings=gt_dataset['embeddings'],\n",
    "    labels=gt_dataset['labels'],\n",
    "    masks=gt_dataset['masks'],\n",
    "    recording_ids=np.array(gt_dataset['recording_ids'], dtype=object),\n",
    "    max_steps=gt_dataset['max_steps']\n",
    ")\n",
    "print(f\"‚úÖ Dataset saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5af03b",
   "metadata": {
    "id": "0e5af03b"
   },
   "source": [
    "## 5. Route B: HiERO Model-based Boundaries\n",
    "\n",
    "This evaluates the **end-to-end system** using step boundaries predicted by HiERO model.\n",
    "\n",
    "Uses hierarchical clustering from HiERO for better step detection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b643e1",
   "metadata": {
    "id": "95b643e1"
   },
   "source": [
    "### 5.1 Setup HiERO Environment\n",
    "\n",
    "**‚ö†Ô∏è IMPORTANT**: Run this cell ONCE at the beginning. It will install all HiERO dependencies from requirements.txt.\n",
    "\n",
    "**Note**: If you see version conflicts (e.g., networkx==3.5), the installer will auto-resolve to a compatible version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1354f8",
   "metadata": {
    "id": "cc1354f8"
   },
   "outputs": [],
   "source": [
    "# Setup HiERO Environment (Fixed Dependencies)\n",
    "import os\n",
    "\n",
    "# Clone HiERO repository\n",
    "if not os.path.exists('/content/HiERO'):\n",
    "    !git clone https://github.com/T-Larm/HiERO_for_egovlp.git /content/HiERO\n",
    "    print(\"‚úÖ HiERO repository cloned\")\n",
    "else:\n",
    "    print(\"‚úÖ HiERO repository already exists\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Installing HiERO Environment (This may take 3-5 minutes)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Install from HiERO's requirements.txt directly\n",
    "# This ensures all dependencies are properly resolved\n",
    "if os.path.exists('/content/HiERO/requirements.txt'):\n",
    "    print(\"\\nüì¶ Installing from HiERO requirements.txt...\")\n",
    "    print(\"   (networkx version will be auto-corrected if needed)\\n\")\n",
    "    \n",
    "    # Install with PyG extra index\n",
    "    !pip install -q -r /content/HiERO/requirements.txt \\\n",
    "        -f https://data.pyg.org/whl/torch-2.4.0+cu124.html \\\n",
    "        --extra-index-url https://download.pytorch.org/whl/cu124 \\\n",
    "        || echo \"‚ö†Ô∏è Some packages may have version conflicts, continuing...\"\n",
    "    \n",
    "    print(\"\\n‚úÖ HiERO requirements installed\")\n",
    "else:\n",
    "    # Fallback: install core dependencies manually\n",
    "    print(\"‚ö†Ô∏è No requirements.txt found, installing core dependencies...\")\n",
    "    !pip install -q torch==2.4.1 torchvision==0.19.1 torchaudio==2.4.1 --index-url https://download.pytorch.org/whl/cu124\n",
    "    !pip install -q torch-geometric torch-scatter torch-sparse -f https://data.pyg.org/whl/torch-2.4.0+cu124.html\n",
    "    !pip install -q einops torch_kmeans tqdm PyYAML networkx scikit-learn timm transformers\n",
    "\n",
    "print(\"\\n‚úÖ Environment setup complete!\")\n",
    "print(\"\\nüìã Verify installation:\")\n",
    "!pip list | grep -E \"torch|networkx|einops|sklearn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b276a3",
   "metadata": {
    "id": "60b276a3"
   },
   "outputs": [],
   "source": [
    "# HiERO configuration\n",
    "USE_HIERO = True  # Set to False to skip HiERO route\n",
    "\n",
    "# HiERO checkpoint path - MODIFY THIS!\n",
    "HIERO_CHECKPOINT = \"/content/drive/MyDrive/AMLproject/hiero_egovlp/hiero_egovlp.pth\"\n",
    "\n",
    "# Check if checkpoint exists\n",
    "if USE_HIERO:\n",
    "    if os.path.exists(HIERO_CHECKPOINT):\n",
    "        print(\"‚úÖ HiERO route enabled\")\n",
    "        print(f\"Checkpoint found: {HIERO_CHECKPOINT}\")\n",
    "        checkpoint_size = os.path.getsize(HIERO_CHECKPOINT) / (1024*1024)\n",
    "        print(f\"Checkpoint size: {checkpoint_size:.1f} MB\")\n",
    "    else:\n",
    "        print(f\"‚ùå Checkpoint not found: {HIERO_CHECKPOINT}\")\n",
    "        print(\"‚ö†Ô∏è Please update HIERO_CHECKPOINT path\")\n",
    "        USE_HIERO = False\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Skipping HiERO route\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62816562",
   "metadata": {
    "id": "62816562"
   },
   "source": [
    "### 5.2 Load HiERO Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba13dc6",
   "metadata": {
    "id": "cba13dc6"
   },
   "outputs": [],
   "source": [
    "if USE_HIERO:\n",
    "    import sys\n",
    "    sys.path.insert(0, '/content/HiERO')\n",
    "\n",
    "    import torch\n",
    "    import yaml\n",
    "    from pathlib import Path\n",
    "    from models.hiero import HiERO\n",
    "    from torch_geometric.data import Data, Batch\n",
    "\n",
    "    # Check device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Load checkpoint\n",
    "    checkpoint = torch.load(HIERO_CHECKPOINT, map_location='cpu')\n",
    "    print(f\"‚úÖ Checkpoint loaded\")\n",
    "    print(f\"Checkpoint keys: {list(checkpoint.keys())}\")\n",
    "\n",
    "    # Try to load config from checkpoint or file\n",
    "    if 'config' in checkpoint:\n",
    "        config = checkpoint['config']\n",
    "        print(\"‚úÖ Config loaded from checkpoint\")\n",
    "    else:\n",
    "        # Try to load from HiERO repo config files\n",
    "        config_path = '/content/HiERO/configs/egovlp.yaml'\n",
    "        if os.path.exists(config_path):\n",
    "            with open(config_path, 'r') as f:\n",
    "                config = yaml.safe_load(f)\n",
    "            print(f\"‚úÖ Config loaded from {config_path}\")\n",
    "        else:\n",
    "            # Use default config\n",
    "            print(\"‚ö†Ô∏è Using default config\")\n",
    "            config = {\n",
    "                'model': {\n",
    "                    'conv': {'name': 'TDGC', 'hidden_size': 256},\n",
    "                    'k': 2.0,\n",
    "                    'n_layers': 2,\n",
    "                    'hidden_size': 256,\n",
    "                    'depth': 3,\n",
    "                    'dropout': 0.1,\n",
    "                    'pool': 'batch_subsampling',\n",
    "                    'n_clusters': 8,\n",
    "                    'clustering_sample_points': 32,\n",
    "                    'clustering_at_inference': False\n",
    "                }\n",
    "            }\n",
    "\n",
    "    # Initialize HiERO model\n",
    "    print(\"\\nInitializing HiERO model...\")\n",
    "    model_config = config.get('model', {})\n",
    "    \n",
    "    # Handle conv config\n",
    "    if 'conv' in model_config:\n",
    "        conv_config = model_config['conv']\n",
    "        # Expand conv config if it's just a string\n",
    "        if isinstance(conv_config, str):\n",
    "            model_config['conv'] = {'name': conv_config}\n",
    "    \n",
    "    hiero_model = HiERO(\n",
    "        input_size=256,  # EgoVLP features are 256-dim\n",
    "        **model_config\n",
    "    )\n",
    "    print(f\"‚úÖ Model initialized\")\n",
    "\n",
    "    # Load state dict\n",
    "    if 'state_dict' in checkpoint:\n",
    "        state_dict = checkpoint['state_dict']\n",
    "    elif 'model' in checkpoint:\n",
    "        state_dict = checkpoint['model']\n",
    "    else:\n",
    "        state_dict = checkpoint\n",
    "\n",
    "    # Try to load weights\n",
    "    try:\n",
    "        # Remove 'module.' prefix if present (from DataParallel)\n",
    "        from collections import OrderedDict\n",
    "        new_state_dict = OrderedDict()\n",
    "        for k, v in state_dict.items():\n",
    "            name = k.replace('module.', '') if k.startswith('module.') else k\n",
    "            new_state_dict[name] = v\n",
    "        \n",
    "        hiero_model.load_state_dict(new_state_dict, strict=True)\n",
    "        print(\"‚úÖ Model weights loaded (strict mode)\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Strict loading failed: {e}\")\n",
    "        try:\n",
    "            hiero_model.load_state_dict(new_state_dict, strict=False)\n",
    "            print(\"‚úÖ Model weights loaded (non-strict mode)\")\n",
    "        except Exception as e2:\n",
    "            print(f\"‚ùå Failed to load weights: {e2}\")\n",
    "            USE_HIERO = False\n",
    "\n",
    "    if USE_HIERO:\n",
    "        hiero_model = hiero_model.to(device)\n",
    "        hiero_model.eval()\n",
    "        \n",
    "        # Count parameters\n",
    "        total_params = sum(p.numel() for p in hiero_model.parameters())\n",
    "        trainable_params = sum(p.numel() for p in hiero_model.parameters() if p.requires_grad)\n",
    "        \n",
    "        print(f\"\\n‚úÖ HiERO model ready!\")\n",
    "        print(f\"   Total parameters: {total_params/1e6:.2f}M\")\n",
    "        print(f\"   Trainable parameters: {trainable_params/1e6:.2f}M\")\n",
    "        print(f\"   Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f46f64",
   "metadata": {
    "id": "50f46f64"
   },
   "source": [
    "### 5.3 HiERO Step Detection Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14c36cd",
   "metadata": {
    "id": "d14c36cd"
   },
   "outputs": [],
   "source": [
    "if USE_HIERO:\n",
    "    from sklearn.cluster import SpectralClustering\n",
    "    import torch.nn.functional as F\n",
    "\n",
    "    def detect_steps_with_hiero(features, model, device, n_clusters='auto', use_spectral=True):\n",
    "        \"\"\"\n",
    "        Detect step boundaries using HiERO model.\n",
    "        \n",
    "        Uses HiERO's hierarchical features + clustering for step detection.\n",
    "\n",
    "        Args:\n",
    "            features: (T, 256) numpy array - EgoVLP features\n",
    "            model: HiERO model\n",
    "            device: torch device\n",
    "            n_clusters: number of clusters ('auto' or int)\n",
    "            use_spectral: use spectral clustering (like HiERO paper)\n",
    "\n",
    "        Returns:\n",
    "            boundaries: list of (start, end) tuples (frame indices)\n",
    "            step_embeddings: (num_steps, 256) array\n",
    "        \"\"\"\n",
    "        T, D = features.shape\n",
    "\n",
    "        # Auto-estimate clusters based on video length\n",
    "        if n_clusters == 'auto':\n",
    "            # Heuristic: ~1 step per 30 seconds (at 1 FPS)\n",
    "            n_clusters = max(3, min(T // 30, 15))\n",
    "            n_clusters = int(n_clusters)\n",
    "\n",
    "        print(f\"  Processing {T} frames ‚Üí {n_clusters} clusters\")\n",
    "\n",
    "        # Convert to torch\n",
    "        x = torch.from_numpy(features).float().to(device)\n",
    "\n",
    "        # Create temporal graph (sequential connections)\n",
    "        edge_index = []\n",
    "        for i in range(T - 1):\n",
    "            edge_index.append([i, i + 1])\n",
    "            edge_index.append([i + 1, i])\n",
    "        edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous().to(device)\n",
    "\n",
    "        # Create PyG graph data\n",
    "        graph_data = Data(x=x, edge_index=edge_index)\n",
    "        graph_data.batch = torch.zeros(T, dtype=torch.long, device=device)\n",
    "        \n",
    "        # HiERO forward pass\n",
    "        with torch.no_grad():\n",
    "            try:\n",
    "                # Forward through HiERO model\n",
    "                output = model(graph_data)\n",
    "                \n",
    "                # Extract features from output\n",
    "                if isinstance(output, Data):\n",
    "                    hiero_features = output.x\n",
    "                elif isinstance(output, dict):\n",
    "                    hiero_features = output.get('x', output.get('features', x))\n",
    "                else:\n",
    "                    hiero_features = output\n",
    "                \n",
    "                # Ensure correct shape\n",
    "                if len(hiero_features) > T:\n",
    "                    hiero_features = hiero_features[:T]\n",
    "                \n",
    "                hiero_features = hiero_features.cpu().numpy()\n",
    "                print(f\"  ‚úì HiERO forward pass: {hiero_features.shape}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  ‚ö†Ô∏è HiERO forward failed ({e}), using original features\")\n",
    "                hiero_features = features\n",
    "\n",
    "        # Normalize features for clustering\n",
    "        features_norm = hiero_features / (np.linalg.norm(hiero_features, axis=1, keepdims=True) + 1e-8)\n",
    "\n",
    "        # Clustering on HiERO features\n",
    "        if use_spectral:\n",
    "            # Spectral clustering (as in HiERO paper)\n",
    "            affinity = features_norm @ features_norm.T\n",
    "            clustering = SpectralClustering(\n",
    "                n_clusters=n_clusters,\n",
    "                affinity='precomputed',\n",
    "                random_state=42,\n",
    "                assign_labels='kmeans'\n",
    "            )\n",
    "            labels = clustering.fit_predict(affinity)\n",
    "            print(f\"  ‚úì Spectral clustering done\")\n",
    "        else:\n",
    "            # KMeans clustering\n",
    "            from sklearn.cluster import KMeans\n",
    "            kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "            labels = kmeans.fit_predict(hiero_features)\n",
    "            print(f\"  ‚úì KMeans clustering done\")\n",
    "\n",
    "        # Convert cluster labels to temporal segments\n",
    "        boundaries = []\n",
    "        current_start = 0\n",
    "        current_label = labels[0]\n",
    "\n",
    "        for i in range(1, len(labels)):\n",
    "            if labels[i] != current_label:\n",
    "                # Minimum step length: 5 frames\n",
    "                if i - current_start >= 5:\n",
    "                    boundaries.append((current_start, i - 1))\n",
    "                    current_start = i\n",
    "                    current_label = labels[i]\n",
    "\n",
    "        # Add last segment\n",
    "        if len(labels) - current_start >= 5:\n",
    "            boundaries.append((current_start, len(labels) - 1))\n",
    "\n",
    "        # Merge very short segments with neighbors\n",
    "        if len(boundaries) > 1:\n",
    "            merged_boundaries = []\n",
    "            i = 0\n",
    "            while i < len(boundaries):\n",
    "                start, end = boundaries[i]\n",
    "                duration = end - start + 1\n",
    "                \n",
    "                # If segment too short, merge with next\n",
    "                if duration < 5 and i < len(boundaries) - 1:\n",
    "                    next_start, next_end = boundaries[i + 1]\n",
    "                    merged_boundaries.append((start, next_end))\n",
    "                    i += 2\n",
    "                else:\n",
    "                    merged_boundaries.append((start, end))\n",
    "                    i += 1\n",
    "            boundaries = merged_boundaries\n",
    "\n",
    "        # Extract step-level embeddings (average pooling)\n",
    "        step_embeddings = []\n",
    "        for start, end in boundaries:\n",
    "            step_emb = hiero_features[start:end+1].mean(axis=0)\n",
    "            step_embeddings.append(step_emb)\n",
    "\n",
    "        step_embeddings = np.stack(step_embeddings, axis=0) if step_embeddings else np.zeros((0, 256))\n",
    "\n",
    "        print(f\"  ‚úì Detected {len(boundaries)} steps\")\n",
    "        return boundaries, step_embeddings\n",
    "\n",
    "    print(\"‚úÖ HiERO detection function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c429c9ef",
   "metadata": {
    "id": "c429c9ef"
   },
   "source": [
    "### 5.4 Process Videos with HiERO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed214cc",
   "metadata": {
    "id": "1ed214cc"
   },
   "outputs": [],
   "source": [
    "if USE_HIERO:\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    # Process all videos with HiERO\n",
    "    hiero_results = {}\n",
    "    failed_videos = []\n",
    "\n",
    "    print(f\"\\nProcessing {len(common_ids)} videos with HiERO model...\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    for recording_id in tqdm(list(common_ids), desc=\"Processing videos\"):\n",
    "        try:\n",
    "            # Find feature file for this video\n",
    "            feature_files = [f for f in os.listdir(EGOVLP_FEATURES_DIR)\n",
    "                           if f.startswith(recording_id) and f.endswith('.npz')]\n",
    "\n",
    "            if not feature_files:\n",
    "                print(f\"‚ö†Ô∏è No features found for {recording_id}\")\n",
    "                failed_videos.append(recording_id)\n",
    "                continue\n",
    "\n",
    "            # Load EgoVLP features\n",
    "            feature_path = os.path.join(EGOVLP_FEATURES_DIR, feature_files[0])\n",
    "            data = np.load(feature_path)\n",
    "            \n",
    "            # Try different keys\n",
    "            if 'arr_0' in data:\n",
    "                features = data['arr_0']\n",
    "            elif 'features' in data:\n",
    "                features = data['features']\n",
    "            else:\n",
    "                features = data[data.files[0]]\n",
    "\n",
    "            # Detect steps using HiERO\n",
    "            boundaries, step_embeddings = detect_steps_with_hiero(\n",
    "                features, \n",
    "                hiero_model, \n",
    "                device,\n",
    "                n_clusters='auto',\n",
    "                use_spectral=True\n",
    "            )\n",
    "\n",
    "            # Get video label from annotations\n",
    "            anno = annotations[recording_id]\n",
    "            has_errors = any(step.get('has_errors', False) for step in anno.get('steps', []))\n",
    "            video_label = 1 if has_errors else 0\n",
    "\n",
    "            # Store results\n",
    "            hiero_results[recording_id] = {\n",
    "                'boundaries': boundaries,\n",
    "                'step_embeddings': step_embeddings,\n",
    "                'video_label': video_label,\n",
    "                'activity_name': anno.get('activity_name', 'unknown'),\n",
    "                'num_frames': len(features)\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"\\n‚ùå Error processing {recording_id}: {e}\")\n",
    "            failed_videos.append(recording_id)\n",
    "            continue\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(f\"‚úÖ Successfully processed: {len(hiero_results)} videos\")\n",
    "    if failed_videos:\n",
    "        print(f\"‚ùå Failed: {len(failed_videos)} videos\")\n",
    "        print(f\"   Failed IDs: {failed_videos[:5]}{'...' if len(failed_videos) > 5 else ''}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6983fc33",
   "metadata": {
    "id": "6983fc33"
   },
   "source": [
    "### 5.5 HiERO Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f033176b",
   "metadata": {
    "id": "f033176b"
   },
   "outputs": [],
   "source": [
    "if USE_HIERO:\n",
    "    # Statistics\n",
    "    hiero_num_steps = [len(v['boundaries']) for v in hiero_results.values()]\n",
    "    hiero_labels = [v['video_label'] for v in hiero_results.values()]\n",
    "\n",
    "    print(\"\\n=== Route B Statistics (HiERO Boundaries) ===\")\n",
    "    print(f\"Total videos processed: {len(hiero_results)}\")\n",
    "    print(f\"Videos with errors: {sum(hiero_labels)} ({sum(hiero_labels)/len(hiero_labels)*100:.1f}%)\")\n",
    "    print(f\"Videos without errors: {len(hiero_labels) - sum(hiero_labels)}\")\n",
    "    print(f\"Avg steps per video: {np.mean(hiero_num_steps):.1f}\")\n",
    "    print(f\"Min/Max steps: {min(hiero_num_steps)} / {max(hiero_num_steps)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41fe818",
   "metadata": {
    "id": "b41fe818"
   },
   "source": [
    "### 5.6 Prepare HiERO Dataset for Substep 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eae61fb",
   "metadata": {
    "id": "8eae61fb"
   },
   "outputs": [],
   "source": [
    "if USE_HIERO:\n",
    "    # Prepare padded dataset\n",
    "    max_steps_hiero = max(len(v['boundaries']) for v in hiero_results.values())\n",
    "    print(f\"Max steps in HiERO dataset: {max_steps_hiero}\")\n",
    "\n",
    "    hiero_embeddings = []\n",
    "    hiero_masks = []\n",
    "    hiero_labels = []\n",
    "    hiero_ids = []\n",
    "\n",
    "    for recording_id, data in hiero_results.items():\n",
    "        step_emb = data['step_embeddings']  # (num_steps, 256)\n",
    "        num_steps = len(step_emb)\n",
    "\n",
    "        # Pad embeddings\n",
    "        padded_emb = np.zeros((max_steps_hiero, 256), dtype=np.float32)\n",
    "        mask = np.zeros(max_steps_hiero, dtype=bool)\n",
    "\n",
    "        padded_emb[:num_steps] = step_emb\n",
    "        mask[:num_steps] = True\n",
    "\n",
    "        hiero_embeddings.append(padded_emb)\n",
    "        hiero_masks.append(mask)\n",
    "        hiero_labels.append(data['video_label'])\n",
    "        hiero_ids.append(recording_id)\n",
    "\n",
    "    # Stack into arrays\n",
    "    hiero_dataset = {\n",
    "        'embeddings': np.stack(hiero_embeddings, axis=0),  # (N, max_steps, 256)\n",
    "        'labels': np.array(hiero_labels),                   # (N,)\n",
    "        'masks': np.stack(hiero_masks, axis=0),             # (N, max_steps)\n",
    "        'recording_ids': hiero_ids,\n",
    "        'max_steps': max_steps_hiero\n",
    "    }\n",
    "\n",
    "    print(f\"\\n=== HiERO Dataset Ready for Substep 2 ===\")\n",
    "    print(f\"Embeddings shape: {hiero_dataset['embeddings'].shape}\")\n",
    "    print(f\"Labels shape: {hiero_dataset['labels'].shape}\")\n",
    "    print(f\"Masks shape: {hiero_dataset['masks'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e77e7f1",
   "metadata": {
    "id": "4e77e7f1"
   },
   "outputs": [],
   "source": [
    "if USE_HIERO:\n",
    "    # Save HiERO dataset\n",
    "    hiero_output_path = os.path.join(OUTPUT_DIR, \"hiero_step_embeddings.npz\")\n",
    "    np.savez(\n",
    "        hiero_output_path,\n",
    "        embeddings=hiero_dataset['embeddings'],\n",
    "        labels=hiero_dataset['labels'],\n",
    "        masks=hiero_dataset['masks'],\n",
    "        recording_ids=np.array(hiero_dataset['recording_ids'], dtype=object),\n",
    "        max_steps=hiero_dataset['max_steps']\n",
    "    )\n",
    "    print(f\"‚úÖ HiERO dataset saved to: {hiero_output_path}\")\n",
    "\n",
    "    # Also save boundaries as JSON\n",
    "    boundaries_json = {\n",
    "        rec_id: {\n",
    "            'boundaries': [(int(s), int(e)) for s, e in data['boundaries']],\n",
    "            'num_steps': len(data['boundaries']),\n",
    "            'video_label': int(data['video_label']),\n",
    "            'activity': data['activity_name']\n",
    "        }\n",
    "        for rec_id, data in hiero_results.items()\n",
    "    }\n",
    "\n",
    "    boundaries_path = os.path.join(OUTPUT_DIR, \"hiero_step_boundaries.json\")\n",
    "    with open(boundaries_path, 'w') as f:\n",
    "        json.dump(boundaries_json, f, indent=2)\n",
    "    print(f\"‚úÖ HiERO boundaries saved to: {boundaries_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e68b070",
   "metadata": {
    "id": "0e68b070"
   },
   "source": [
    "### 5.7 Compare GT vs HiERO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f29d9f",
   "metadata": {
    "id": "a0f29d9f"
   },
   "outputs": [],
   "source": [
    "if USE_HIERO:\n",
    "    # Compare GT vs HiERO detected steps\n",
    "    comparison = []\n",
    "\n",
    "    for recording_id in common_ids:\n",
    "        if recording_id not in hiero_results:\n",
    "            continue\n",
    "\n",
    "        gt_video = gt_results.get(recording_id)\n",
    "        hiero_data = hiero_results.get(recording_id)\n",
    "\n",
    "        if gt_video and hiero_data:\n",
    "            gt_num_steps = len(gt_video.steps)\n",
    "            hiero_num_steps = len(hiero_data['boundaries'])\n",
    "\n",
    "            comparison.append({\n",
    "                'recording_id': recording_id,\n",
    "                'gt_steps': gt_num_steps,\n",
    "                'hiero_steps': hiero_num_steps,\n",
    "                'difference': hiero_num_steps - gt_num_steps,\n",
    "                'video_label': hiero_data['video_label']\n",
    "            })\n",
    "\n",
    "    # Statistics\n",
    "    differences = [c['difference'] for c in comparison]\n",
    "    print(f\"\\n=== GT vs HiERO Comparison ===\")\n",
    "    print(f\"Videos compared: {len(comparison)}\")\n",
    "    print(f\"Average difference (HiERO - GT): {np.mean(differences):.2f} steps\")\n",
    "    print(f\"Std dev: {np.std(differences):.2f}\")\n",
    "    print(f\"Min/Max difference: {min(differences)} / {max(differences)}\")\n",
    "\n",
    "    # Save comparison\n",
    "    comparison_path = os.path.join(OUTPUT_DIR, \"gt_vs_hiero_comparison.json\")\n",
    "    with open(comparison_path, 'w') as f:\n",
    "        json.dump(comparison, f, indent=2)\n",
    "    print(f\"‚úÖ Comparison saved to: {comparison_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06564690",
   "metadata": {
    "id": "06564690"
   },
   "source": [
    "## 6. Summary & Next Steps\n",
    "\n",
    "### What we have now:\n",
    "1. **GT Step Embeddings** (`gt_step_embeddings.npz`) - Upper bound baseline\n",
    "   - Shape: `(N, max_steps, 256)`\n",
    "   - Uses perfect step boundaries from annotations\n",
    "   \n",
    "2. **HiERO Step Embeddings** (`hiero_step_embeddings.npz`) - Predicted boundaries\n",
    "   - Shape: `(N, max_steps, 256)`\n",
    "   - Uses HiERO model for step detection\n",
    "   - More realistic end-to-end system performance\n",
    "\n",
    "3. **HiERO Boundaries** (`hiero_step_boundaries.json`)\n",
    "   - Predicted step boundaries for each video\n",
    "   - Ready for Substeps 3 & 4 (Task Graph matching)\n",
    "\n",
    "### Next Steps:\n",
    "1. **Substep 2**: Train Transformer classifier on step embeddings\n",
    "   - Test with both GT (upper bound) and HiERO (realistic) embeddings\n",
    "2. **Substep 3**: Encode task graph nodes, match with HiERO visual features\n",
    "3. **Substep 4**: Train GNN classifier on matched task graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25f8bcd",
   "metadata": {
    "id": "a25f8bcd"
   },
   "outputs": [],
   "source": [
    "# Summary\n",
    "print(\"=\"*60)\n",
    "print(\"Extension Substep 1 Complete!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nüìä Results saved to: {OUTPUT_DIR}\")\n",
    "print(f\"\\nüìÅ Files created:\")\n",
    "for f in os.listdir(OUTPUT_DIR):\n",
    "    fpath = os.path.join(OUTPUT_DIR, f)\n",
    "    size = os.path.getsize(fpath) / (1024*1024)  # MB\n",
    "    print(f\"   - {f} ({size:.2f} MB)\")\n",
    "\n",
    "print(f\"\\nüéØ Summary:\")\n",
    "print(f\"   - GT baseline: {len(gt_results)} videos (upper bound)\")\n",
    "if USE_HIERO:\n",
    "    print(f\"   - HiERO predictions: {len(hiero_results)} videos (realistic)\")\n",
    "    print(f\"   - Average steps - GT: {np.mean(num_steps_list):.1f}, HiERO: {np.mean(hiero_num_steps):.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee62141c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üí° Alternative: Install ALL dependencies at once (Recommended)\n",
    "\n",
    "If you want to set up the **complete environment in one go**, uncomment and run this cell instead of installing dependencies in multiple steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86062e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # OPTION: One-shot environment setup\n",
    "# # Uncomment this cell to install everything at once\n",
    "# \n",
    "# print(\"=\"*70)\n",
    "# print(\"ONE-SHOT ENVIRONMENT SETUP\")\n",
    "# print(\"=\"*70)\n",
    "# \n",
    "# # 1. Clone repositories\n",
    "# !git clone https://github.com/T-Larm/aml-2025-mistake-detection-gp.git /content/aml-2025-mistake-detection-gp\n",
    "# %cd /content/aml-2025-mistake-detection-gp\n",
    "# !git submodule update --init --recursive\n",
    "# \n",
    "# !git clone https://github.com/T-Larm/HiERO_for_egovlp.git /content/HiERO\n",
    "# \n",
    "# # 2. Install project requirements\n",
    "# print(\"\\nüì¶ Installing project requirements...\")\n",
    "# !pip install -q -r requirements.txt\n",
    "# \n",
    "# # 3. Install HiERO requirements\n",
    "# print(\"\\nüì¶ Installing HiERO requirements...\")\n",
    "# !pip install -q -r /content/HiERO/requirements.txt \\\n",
    "#     -f https://data.pyg.org/whl/torch-2.4.0+cu124.html \\\n",
    "#     --extra-index-url https://download.pytorch.org/whl/cu124 \\\n",
    "#     || echo \"‚ö†Ô∏è Version conflicts auto-resolved\"\n",
    "# \n",
    "# # 4. Mount Drive\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# \n",
    "# print(\"\\n‚úÖ Complete environment ready!\")\n",
    "# print(\"\\n‚è≠Ô∏è  You can now skip to Section 2 (Path Configuration)\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
